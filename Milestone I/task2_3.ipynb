{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: Arya Ramesh Patil\n",
    "#### Student ID: S4060675\n",
    "\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* numpy\n",
    "* CountVectorizer\n",
    "* TfidfVectorizer\n",
    "* gensim.downloader\n",
    "* train_test_split\n",
    "* LogisticRegression\n",
    "* KFold\n",
    "* RegexpTokenizer\n",
    "* sent_tokenize\n",
    "* chain\n",
    "* division\n",
    "\n",
    "## Introduction\n",
    "This part of the assessment focuses on generating feature representations for clothing reviews and using them to classify item recommendations. Task 2 involves creation of vector representations and word embeddings (weighted and unweighted) based on one of the said models. I have chosen pre-trained glove model (glove-wiki-gigaword-50). These representations capture the essential information from the reviews for further analysis. Task 3 involves building machine learning models to classify whether an clothing item is recommended based on a review. The task also involves, experimenting with 3 different types of feature representations (count feature, weighted and unweighted features) to determine which one performs best and to evaluate if adding extra information such as 'Review Title' improves classification accuracy. The column 'Review Title' is pre-processed the same way as 'Review Text' in Task 1. To get robust results, the classification is performed using Kfolds (where number of folds are set at 5 by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "from __future__ import division\n",
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Clothing Items Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task involves creating 3 models namely:\n",
    "* Count Vector Representation\n",
    "* Unweighted Embedding\n",
    "* Weighted Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning file name to csv_file variable\n",
    "csv_file = 'processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the said csv file into a dataframe\n",
    "clothes_review_data = pd.read_csv(csv_file, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Processed Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>['high', 'hopes', 'wanted', 'work', 'initially...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>['jumpsuit', 'fun', 'flirty', 'fabulous', 'tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>['shirt', 'due', 'adjustable', 'front', 'tie',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>['tracy', 'reese', 'dresses', 'petite', 'feet'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>['basket', 'hte', 'person', 'store', 'pick', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>['happy', 'snag', 'price', 'easy', 'slip', 'cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>['reminds', 'maternity', 'clothes', 'stretchy'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19649</th>\n",
       "      <td>1104</td>\n",
       "      <td>31</td>\n",
       "      <td>Cute, but see through</td>\n",
       "      <td>This fit well, but the top was very see throug...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>['worked', 'glad', 'store', 'order', 'online']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19650</th>\n",
       "      <td>1084</td>\n",
       "      <td>28</td>\n",
       "      <td>Very cute dress, perfect for summer parties an...</td>\n",
       "      <td>I bought this dress for a wedding i have this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>['wedding', 'summer', 'medium', 'waist', 'perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19651</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>['lovely', 'feminine', 'perfectly', 'easy', 'c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19652 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                                              Title  \\\n",
       "0             1077   60                            Some major design flaws   \n",
       "1             1049   50                                   My favorite buy!   \n",
       "2              847   47                                   Flattering shirt   \n",
       "3             1080   49                            Not for the very petite   \n",
       "4              858   39                               Cagrcoal shimmer fun   \n",
       "...            ...  ...                                                ...   \n",
       "19647         1104   34                     Great dress for many occasions   \n",
       "19648          862   48                         Wish it was made of cotton   \n",
       "19649         1104   31                              Cute, but see through   \n",
       "19650         1084   28  Very cute dress, perfect for summer parties an...   \n",
       "19651         1104   52                    Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "19647  I was very happy to snag this dress at such a ...       5   \n",
       "19648  It reminds me of maternity clothes. soft, stre...       3   \n",
       "19649  This fit well, but the top was very see throug...       3   \n",
       "19650  I bought this dress for a wedding i have this ...       3   \n",
       "19651  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "19647                1                        0  General Petite   \n",
       "19648                1                        0  General Petite   \n",
       "19649                0                        1  General Petite   \n",
       "19650                1                        2         General   \n",
       "19651                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name  \\\n",
       "0             Dresses    Dresses   \n",
       "1             Bottoms      Pants   \n",
       "2                Tops    Blouses   \n",
       "3             Dresses    Dresses   \n",
       "4                Tops      Knits   \n",
       "...               ...        ...   \n",
       "19647         Dresses    Dresses   \n",
       "19648            Tops      Knits   \n",
       "19649         Dresses    Dresses   \n",
       "19650         Dresses    Dresses   \n",
       "19651         Dresses    Dresses   \n",
       "\n",
       "                                   Processed Review Text  \n",
       "0      ['high', 'hopes', 'wanted', 'work', 'initially...  \n",
       "1      ['jumpsuit', 'fun', 'flirty', 'fabulous', 'tim...  \n",
       "2      ['shirt', 'due', 'adjustable', 'front', 'tie',...  \n",
       "3      ['tracy', 'reese', 'dresses', 'petite', 'feet'...  \n",
       "4      ['basket', 'hte', 'person', 'store', 'pick', '...  \n",
       "...                                                  ...  \n",
       "19647  ['happy', 'snag', 'price', 'easy', 'slip', 'cu...  \n",
       "19648  ['reminds', 'maternity', 'clothes', 'stretchy'...  \n",
       "19649     ['worked', 'glad', 'store', 'order', 'online']  \n",
       "19650  ['wedding', 'summer', 'medium', 'waist', 'perf...  \n",
       "19651  ['lovely', 'feminine', 'perfectly', 'easy', 'c...  \n",
       "\n",
       "[19652 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the data\n",
    "clothes_review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving the file as 'processed.csv' in Task 1, the tokens were not enclosed in quotations. However, when the file is opened in Excel, the tokens are automatically stored as strings, adding quotations around them. As a result, when reading the 'processed.csv' file for subsequent tasks in Task 2, the tokens appear with quotations. To work with the tokens without the quotations, I found through further research that the eval() function can be used to remove the quotes and restore the original token format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing quotations around the review tokens\n",
    "clothes_review_data['Processed Review Text'] = clothes_review_data['Processed Review Text'].apply(eval) # [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Processed Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[high, hopes, wanted, work, initially, petite,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>[jumpsuit, fun, flirty, fabulous, time, compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>[shirt, due, adjustable, front, tie, length, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[tracy, reese, dresses, petite, feet, tall, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[basket, hte, person, store, pick, teh, pale, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[happy, snag, price, easy, slip, cut, combo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[reminds, maternity, clothes, stretchy, shiny,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19649</th>\n",
       "      <td>1104</td>\n",
       "      <td>31</td>\n",
       "      <td>Cute, but see through</td>\n",
       "      <td>This fit well, but the top was very see throug...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[worked, glad, store, order, online]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19650</th>\n",
       "      <td>1084</td>\n",
       "      <td>28</td>\n",
       "      <td>Very cute dress, perfect for summer parties an...</td>\n",
       "      <td>I bought this dress for a wedding i have this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[wedding, summer, medium, waist, perfectly, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19651</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[lovely, feminine, perfectly, easy, comfy, hig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19652 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                                              Title  \\\n",
       "0             1077   60                            Some major design flaws   \n",
       "1             1049   50                                   My favorite buy!   \n",
       "2              847   47                                   Flattering shirt   \n",
       "3             1080   49                            Not for the very petite   \n",
       "4              858   39                               Cagrcoal shimmer fun   \n",
       "...            ...  ...                                                ...   \n",
       "19647         1104   34                     Great dress for many occasions   \n",
       "19648          862   48                         Wish it was made of cotton   \n",
       "19649         1104   31                              Cute, but see through   \n",
       "19650         1084   28  Very cute dress, perfect for summer parties an...   \n",
       "19651         1104   52                    Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "19647  I was very happy to snag this dress at such a ...       5   \n",
       "19648  It reminds me of maternity clothes. soft, stre...       3   \n",
       "19649  This fit well, but the top was very see throug...       3   \n",
       "19650  I bought this dress for a wedding i have this ...       3   \n",
       "19651  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "19647                1                        0  General Petite   \n",
       "19648                1                        0  General Petite   \n",
       "19649                0                        1  General Petite   \n",
       "19650                1                        2         General   \n",
       "19651                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name  \\\n",
       "0             Dresses    Dresses   \n",
       "1             Bottoms      Pants   \n",
       "2                Tops    Blouses   \n",
       "3             Dresses    Dresses   \n",
       "4                Tops      Knits   \n",
       "...               ...        ...   \n",
       "19647         Dresses    Dresses   \n",
       "19648            Tops      Knits   \n",
       "19649         Dresses    Dresses   \n",
       "19650         Dresses    Dresses   \n",
       "19651         Dresses    Dresses   \n",
       "\n",
       "                                   Processed Review Text  \n",
       "0      [high, hopes, wanted, work, initially, petite,...  \n",
       "1      [jumpsuit, fun, flirty, fabulous, time, compli...  \n",
       "2      [shirt, due, adjustable, front, tie, length, l...  \n",
       "3      [tracy, reese, dresses, petite, feet, tall, br...  \n",
       "4      [basket, hte, person, store, pick, teh, pale, ...  \n",
       "...                                                  ...  \n",
       "19647       [happy, snag, price, easy, slip, cut, combo]  \n",
       "19648  [reminds, maternity, clothes, stretchy, shiny,...  \n",
       "19649               [worked, glad, store, order, online]  \n",
       "19650  [wedding, summer, medium, waist, perfectly, lo...  \n",
       "19651  [lovely, feminine, perfectly, easy, comfy, hig...  \n",
       "\n",
       "[19652 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clothes_review_data # checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting 'Processed Review Text' in 'review_text' variable\n",
    "review_text = clothes_review_data['Processed Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the vocabulary file\n",
    "vocab_file = \"vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'vocab' is stored as a dictionary because dictionaries allow efficient mapping between words (keys) and their corresponding integer indices (values). This is useful for quickly looking up the index of a word when processing text. Moreover, the format of the content naturally fits the dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {} # initialising an empty dictionary\n",
    "with open(vocab_file, 'r') as f: # opening the vocabulary file in read mode\n",
    "    for line in f: # looping through each line\n",
    "        word, index = line.strip().split(':') # stripping the leading/trailing whitespaces and splitting on ':'\n",
    "        vocab[word] = int(index) # reading it in said format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a-cup': 0,\n",
       " 'a-flutter': 1,\n",
       " 'a-frame': 2,\n",
       " 'a-kind': 3,\n",
       " 'a-line': 4,\n",
       " 'a-lines': 5,\n",
       " 'a-symmetric': 6,\n",
       " 'aa': 7,\n",
       " 'ab': 8,\n",
       " 'abbey': 9,\n",
       " 'abby': 10,\n",
       " 'abdomen': 11,\n",
       " 'ability': 12,\n",
       " 'abnormally': 13,\n",
       " 'abo': 14,\n",
       " 'abou': 15,\n",
       " 'above-the': 16,\n",
       " 'abroad': 17,\n",
       " 'abs': 18,\n",
       " 'absolute': 19,\n",
       " 'absolutely': 20,\n",
       " 'absolutley': 21,\n",
       " 'absolutly': 22,\n",
       " 'abstract': 23,\n",
       " 'absurd': 24,\n",
       " 'abt': 25,\n",
       " 'abundance': 26,\n",
       " 'ac': 27,\n",
       " 'accent': 28,\n",
       " 'accented': 29,\n",
       " 'accenting': 30,\n",
       " 'accents': 31,\n",
       " 'accentuate': 32,\n",
       " 'accentuated': 33,\n",
       " 'accentuates': 34,\n",
       " 'accentuating': 35,\n",
       " 'accept': 36,\n",
       " 'acceptable': 37,\n",
       " 'accepted': 38,\n",
       " 'access': 39,\n",
       " 'accessories': 40,\n",
       " 'accessorize': 41,\n",
       " 'accessorized': 42,\n",
       " 'accessorizing': 43,\n",
       " 'accessory': 44,\n",
       " 'accident': 45,\n",
       " 'accidental': 46,\n",
       " 'accidentally': 47,\n",
       " 'accommodate': 48,\n",
       " 'accommodated': 49,\n",
       " 'accommodates': 50,\n",
       " 'accommodating': 51,\n",
       " 'accomodate': 52,\n",
       " 'accompanying': 53,\n",
       " 'accomplish': 54,\n",
       " 'accordian': 55,\n",
       " 'account': 56,\n",
       " 'accurate': 57,\n",
       " 'accurately': 58,\n",
       " 'acetate': 59,\n",
       " 'achieve': 60,\n",
       " 'acrylic': 61,\n",
       " 'act': 62,\n",
       " 'action': 63,\n",
       " 'active': 64,\n",
       " 'activewear': 65,\n",
       " 'activities': 66,\n",
       " 'acts': 67,\n",
       " 'actual': 68,\n",
       " 'actuality': 69,\n",
       " 'ad': 70,\n",
       " 'ada': 71,\n",
       " 'add': 72,\n",
       " 'add-on': 73,\n",
       " 'added': 74,\n",
       " 'addict': 75,\n",
       " 'addicted': 76,\n",
       " 'adding': 77,\n",
       " 'addition': 78,\n",
       " 'additional': 79,\n",
       " 'additionally': 80,\n",
       " 'address': 81,\n",
       " 'adds': 82,\n",
       " 'adequate': 83,\n",
       " 'adequately': 84,\n",
       " 'adjust': 85,\n",
       " 'adjustable': 86,\n",
       " 'adjusted': 87,\n",
       " 'adjusting': 88,\n",
       " 'adjustment': 89,\n",
       " 'adjustments': 90,\n",
       " 'admire': 91,\n",
       " 'admired': 92,\n",
       " 'admiring': 93,\n",
       " 'admit': 94,\n",
       " 'admittedly': 95,\n",
       " 'adn': 96,\n",
       " 'ador': 97,\n",
       " 'adorable': 98,\n",
       " 'adore': 99,\n",
       " 'adored': 100,\n",
       " 'ads': 101,\n",
       " 'adult': 102,\n",
       " 'adults': 103,\n",
       " 'advantage': 104,\n",
       " 'advantages': 105,\n",
       " 'adventure': 106,\n",
       " 'advertised': 107,\n",
       " 'advice': 108,\n",
       " 'advise': 109,\n",
       " 'advised': 110,\n",
       " 'aesthetic': 111,\n",
       " 'aesthetically': 112,\n",
       " 'aesthetics': 113,\n",
       " 'affair': 114,\n",
       " 'affect': 115,\n",
       " 'affected': 116,\n",
       " 'affects': 117,\n",
       " 'afford': 118,\n",
       " 'affordable': 119,\n",
       " 'aforementioned': 120,\n",
       " 'afraid': 121,\n",
       " 'afternoon': 122,\n",
       " 'afterward': 123,\n",
       " 'ag': 124,\n",
       " \"ag's\": 125,\n",
       " 'age': 126,\n",
       " 'age-appropriate': 127,\n",
       " 'aged': 128,\n",
       " 'ages': 129,\n",
       " 'aggressive': 130,\n",
       " 'agin': 131,\n",
       " 'ago': 132,\n",
       " 'agree': 133,\n",
       " 'agreed': 134,\n",
       " 'agreement': 135,\n",
       " 'ags': 136,\n",
       " 'ah': 137,\n",
       " 'ahd': 138,\n",
       " 'ahead': 139,\n",
       " 'ahhh': 140,\n",
       " 'ahs': 141,\n",
       " 'ahve': 142,\n",
       " 'air': 143,\n",
       " 'air-dried': 144,\n",
       " 'air-drying': 145,\n",
       " 'airiness': 146,\n",
       " 'airing': 147,\n",
       " 'airplanes': 148,\n",
       " 'airport': 149,\n",
       " 'airy': 150,\n",
       " 'aize': 151,\n",
       " 'aka': 152,\n",
       " 'akemi': 153,\n",
       " 'al': 154,\n",
       " 'alas': 155,\n",
       " 'alaska': 156,\n",
       " 'albeit': 157,\n",
       " 'alexandria': 158,\n",
       " 'align': 159,\n",
       " 'aligned': 160,\n",
       " 'alike': 161,\n",
       " 'aline': 162,\n",
       " 'alittle': 163,\n",
       " 'all-around': 164,\n",
       " 'all-in': 165,\n",
       " 'all-over': 166,\n",
       " 'alley': 167,\n",
       " 'allison': 168,\n",
       " 'allover': 169,\n",
       " 'allowed': 170,\n",
       " 'allowing': 171,\n",
       " 'allusion': 172,\n",
       " 'alot': 173,\n",
       " 'alpaca': 174,\n",
       " 'alright': 175,\n",
       " 'als': 176,\n",
       " 'alter': 177,\n",
       " 'alteration': 178,\n",
       " 'alterations': 179,\n",
       " 'altered': 180,\n",
       " 'altering': 181,\n",
       " 'alternate': 182,\n",
       " 'alternations': 183,\n",
       " 'alternative': 184,\n",
       " 'altho': 185,\n",
       " 'altogether': 186,\n",
       " 'amadi': 187,\n",
       " 'amalfi': 188,\n",
       " 'amazed': 189,\n",
       " 'amazing': 190,\n",
       " 'amazingly': 191,\n",
       " 'amazon': 192,\n",
       " 'ambiguous': 193,\n",
       " 'amd': 194,\n",
       " 'american': 195,\n",
       " 'amount': 196,\n",
       " 'amounts': 197,\n",
       " 'amp': 198,\n",
       " 'ample': 199,\n",
       " 'amply': 200,\n",
       " 'amterial': 201,\n",
       " 'anatomy': 202,\n",
       " 'and-go': 203,\n",
       " 'angel': 204,\n",
       " 'angeles': 205,\n",
       " 'angle': 206,\n",
       " 'angled': 207,\n",
       " 'angles': 208,\n",
       " 'angora': 209,\n",
       " 'animal': 210,\n",
       " 'animals': 211,\n",
       " 'ankle': 212,\n",
       " 'ankle-length': 213,\n",
       " 'ankles': 214,\n",
       " \"ann's\": 215,\n",
       " 'anna': 216,\n",
       " 'anniversary': 217,\n",
       " 'annoyance': 218,\n",
       " 'annoyed': 219,\n",
       " 'annoying': 220,\n",
       " 'anorak': 221,\n",
       " 'ans': 222,\n",
       " 'answer': 223,\n",
       " 'ant': 224,\n",
       " 'anth': 225,\n",
       " 'anther': 226,\n",
       " 'antho': 227,\n",
       " 'anticipate': 228,\n",
       " 'anticipated': 229,\n",
       " 'anticipating': 230,\n",
       " 'anticipation': 231,\n",
       " 'antique': 232,\n",
       " 'antrho': 233,\n",
       " 'antro': 234,\n",
       " 'anxious': 235,\n",
       " 'anxiously': 236,\n",
       " 'anymore': 237,\n",
       " \"anyone's\": 238,\n",
       " 'anytime': 239,\n",
       " 'app': 240,\n",
       " 'appalled': 241,\n",
       " 'apparel': 242,\n",
       " 'apparent': 243,\n",
       " 'apparently': 244,\n",
       " 'appeal': 245,\n",
       " 'appealed': 246,\n",
       " 'appealing': 247,\n",
       " 'appearance': 248,\n",
       " 'appeared': 249,\n",
       " 'appearing': 250,\n",
       " 'appears': 251,\n",
       " 'apple': 252,\n",
       " 'apple-shaped': 253,\n",
       " 'applied': 254,\n",
       " 'appliqu': 255,\n",
       " 'applique': 256,\n",
       " 'apply': 257,\n",
       " 'appreciated': 258,\n",
       " 'apprehensive': 259,\n",
       " 'approach': 260,\n",
       " 'approaching': 261,\n",
       " 'appropriately': 262,\n",
       " 'approved': 263,\n",
       " 'approx': 264,\n",
       " 'approximate': 265,\n",
       " 'approximately': 266,\n",
       " 'apprx': 267,\n",
       " 'apricot': 268,\n",
       " 'april': 269,\n",
       " 'apron': 270,\n",
       " 'apt': 271,\n",
       " 'aqua': 272,\n",
       " 'ar': 273,\n",
       " 'arc': 274,\n",
       " 'area': 275,\n",
       " 'areas': 276,\n",
       " 'aren': 277,\n",
       " 'arielle': 278,\n",
       " 'arm': 279,\n",
       " 'arm-holes': 280,\n",
       " 'armed': 281,\n",
       " 'armhole': 282,\n",
       " 'armholes': 283,\n",
       " 'armpit': 284,\n",
       " 'armpits': 285,\n",
       " 'arms': 286,\n",
       " 'army': 287,\n",
       " 'arranged': 288,\n",
       " 'arrival': 289,\n",
       " 'arrivals': 290,\n",
       " 'arrive': 291,\n",
       " 'arrived': 292,\n",
       " 'arrives': 293,\n",
       " 'arrows': 294,\n",
       " 'art': 295,\n",
       " 'article': 296,\n",
       " 'articles': 297,\n",
       " 'artist': 298,\n",
       " \"artist's\": 299,\n",
       " 'artistic': 300,\n",
       " 'artsy': 301,\n",
       " 'artwork': 302,\n",
       " 'arty-looking': 303,\n",
       " 'as-is': 304,\n",
       " 'as-pictured': 305,\n",
       " 'asap': 306,\n",
       " 'ashley': 307,\n",
       " 'asia': 308,\n",
       " 'asked': 309,\n",
       " 'aspect': 310,\n",
       " 'aspects': 311,\n",
       " 'assessing': 312,\n",
       " 'assets': 313,\n",
       " 'assistance': 314,\n",
       " 'assistant': 315,\n",
       " 'associate': 316,\n",
       " 'associates': 317,\n",
       " 'assume': 318,\n",
       " 'assumed': 319,\n",
       " 'assuming': 320,\n",
       " 'assumption': 321,\n",
       " 'assured': 322,\n",
       " 'astounded': 323,\n",
       " 'asymmetric': 324,\n",
       " 'asymmetrical': 325,\n",
       " 'asymmetry': 326,\n",
       " 'ate': 327,\n",
       " 'athleisure': 328,\n",
       " 'athlete': 329,\n",
       " 'athletic': 330,\n",
       " 'atl': 331,\n",
       " 'atlanta': 332,\n",
       " 'atleast': 333,\n",
       " 'atrocious': 334,\n",
       " 'attach': 335,\n",
       " 'attached': 336,\n",
       " 'attaches': 337,\n",
       " 'attachment': 338,\n",
       " 'attempt': 339,\n",
       " 'attempted': 340,\n",
       " 'attempting': 341,\n",
       " 'attend': 342,\n",
       " 'attendant': 343,\n",
       " 'attended': 344,\n",
       " 'attending': 345,\n",
       " 'attention': 346,\n",
       " 'attire': 347,\n",
       " 'attitude': 348,\n",
       " 'attracted': 349,\n",
       " 'attracting': 350,\n",
       " 'attractive': 351,\n",
       " 'audrey': 352,\n",
       " 'august': 353,\n",
       " 'aunt': 354,\n",
       " 'australian': 355,\n",
       " 'authentic': 356,\n",
       " 'automatically': 357,\n",
       " 'autumn': 358,\n",
       " 'autumnal': 359,\n",
       " 'avail': 360,\n",
       " 'availability': 361,\n",
       " 'average': 362,\n",
       " 'avid': 363,\n",
       " 'avoid': 364,\n",
       " 'avoided': 365,\n",
       " 'avoiding': 366,\n",
       " 'avoids': 367,\n",
       " 'awaited': 368,\n",
       " 'awaiting': 369,\n",
       " 'awards': 370,\n",
       " 'aware': 371,\n",
       " 'awesome': 372,\n",
       " 'awful': 373,\n",
       " 'awhile': 374,\n",
       " 'awkward': 375,\n",
       " 'awkwardly': 376,\n",
       " 'awkwardness': 377,\n",
       " 'az': 378,\n",
       " 'b-c': 379,\n",
       " 'ba': 380,\n",
       " 'babies': 381,\n",
       " 'baby': 382,\n",
       " 'baby-doll': 383,\n",
       " 'babydoll': 384,\n",
       " 'bac': 385,\n",
       " 'bachelorette': 386,\n",
       " 'back-ordered': 387,\n",
       " 'back-up': 388,\n",
       " 'backed': 389,\n",
       " 'background': 390,\n",
       " 'backorder': 391,\n",
       " 'backordered': 392,\n",
       " 'backpack': 393,\n",
       " 'backs': 394,\n",
       " 'backside': 395,\n",
       " 'backup': 396,\n",
       " 'backward': 397,\n",
       " 'backwards': 398,\n",
       " 'backyard': 399,\n",
       " 'bad': 400,\n",
       " 'badly': 401,\n",
       " 'bag': 402,\n",
       " 'bagged': 403,\n",
       " 'baggie': 404,\n",
       " 'baggier': 405,\n",
       " 'bagginess': 406,\n",
       " 'bagging': 407,\n",
       " 'baggy': 408,\n",
       " 'bags': 409,\n",
       " 'bailey': 410,\n",
       " 'baily': 411,\n",
       " 'balance': 412,\n",
       " 'balanced': 413,\n",
       " 'balances': 414,\n",
       " 'balck': 415,\n",
       " 'balked': 416,\n",
       " 'ball': 417,\n",
       " 'ballet': 418,\n",
       " 'balloon': 419,\n",
       " 'ballooned': 420,\n",
       " 'balloons': 421,\n",
       " 'balloony': 422,\n",
       " 'balls': 423,\n",
       " 'bam': 424,\n",
       " 'band': 425,\n",
       " 'bandage': 426,\n",
       " 'bandeau': 427,\n",
       " 'banded': 428,\n",
       " 'banding': 429,\n",
       " 'bands': 430,\n",
       " 'bank': 431,\n",
       " 'bar': 432,\n",
       " 'barbecue': 433,\n",
       " 'bare': 434,\n",
       " 'barefoot': 435,\n",
       " 'barely': 436,\n",
       " 'bargain': 437,\n",
       " 'barley': 438,\n",
       " 'baroque': 439,\n",
       " 'barre': 440,\n",
       " 'barrel': 441,\n",
       " 'base': 442,\n",
       " 'baseball': 443,\n",
       " 'based': 444,\n",
       " 'basic': 445,\n",
       " 'basically': 446,\n",
       " 'basics': 447,\n",
       " 'basis': 448,\n",
       " 'basket': 449,\n",
       " 'basketball': 450,\n",
       " 'basketweave': 451,\n",
       " 'bat': 452,\n",
       " 'batch': 453,\n",
       " 'batches': 454,\n",
       " 'bath': 455,\n",
       " 'bathing': 456,\n",
       " 'bathrobe': 457,\n",
       " 'bathroom': 458,\n",
       " 'battle': 459,\n",
       " 'batwing': 460,\n",
       " 'batwings': 461,\n",
       " 'bay': 462,\n",
       " 'bbq': 463,\n",
       " 'bc': 464,\n",
       " 'bday': 465,\n",
       " 'bea': 466,\n",
       " 'beach': 467,\n",
       " 'beachy': 468,\n",
       " 'bead': 469,\n",
       " 'beaded': 470,\n",
       " 'beading': 471,\n",
       " 'beads': 472,\n",
       " 'beadwork': 473,\n",
       " 'bear': 474,\n",
       " 'beat': 475,\n",
       " 'beauties': 476,\n",
       " 'beautifu': 477,\n",
       " 'beautiful': 478,\n",
       " 'beautifully': 479,\n",
       " 'beautify': 480,\n",
       " 'beauty': 481,\n",
       " 'bec': 482,\n",
       " 'beca': 483,\n",
       " 'becasue': 484,\n",
       " 'becau': 485,\n",
       " 'becaus': 486,\n",
       " 'becuase': 487,\n",
       " 'bed': 488,\n",
       " 'bedtime': 489,\n",
       " 'bee': 490,\n",
       " 'beef': 491,\n",
       " 'bees': 492,\n",
       " 'began': 493,\n",
       " 'begging': 494,\n",
       " 'begin': 495,\n",
       " 'beginning': 496,\n",
       " 'begins': 497,\n",
       " 'behold': 498,\n",
       " 'beige': 499,\n",
       " 'bein': 500,\n",
       " 'bel': 501,\n",
       " 'bell': 502,\n",
       " 'bell-sleeve': 503,\n",
       " 'belled': 504,\n",
       " 'bellow': 505,\n",
       " 'bells': 506,\n",
       " 'belly': 507,\n",
       " 'bellybutton': 508,\n",
       " 'belonged': 509,\n",
       " 'beloved': 510,\n",
       " 'belt': 511,\n",
       " 'belted': 512,\n",
       " 'belting': 513,\n",
       " 'belts': 514,\n",
       " 'bend': 515,\n",
       " 'bending': 516,\n",
       " 'beneath': 517,\n",
       " 'benefit': 518,\n",
       " 'bent': 519,\n",
       " 'bermuda': 520,\n",
       " 'berry': 521,\n",
       " 'bes': 522,\n",
       " 'bet': 523,\n",
       " 'bette': 524,\n",
       " 'betty': 525,\n",
       " 'beware': 526,\n",
       " 'bf': 527,\n",
       " 'bff': 528,\n",
       " 'bi': 529,\n",
       " 'bias': 530,\n",
       " 'biased': 531,\n",
       " 'bib': 532,\n",
       " 'bicep': 533,\n",
       " 'biceps': 534,\n",
       " 'bicycle': 535,\n",
       " 'bid': 536,\n",
       " 'big': 537,\n",
       " 'bigger': 538,\n",
       " 'biggest': 539,\n",
       " 'biggie': 540,\n",
       " 'biggish': 541,\n",
       " 'biker': 542,\n",
       " 'bikini': 543,\n",
       " 'bikinis': 544,\n",
       " 'bill': 545,\n",
       " 'billow': 546,\n",
       " 'billowed': 547,\n",
       " 'billowing': 548,\n",
       " 'billows': 549,\n",
       " 'billowy': 550,\n",
       " 'bind': 551,\n",
       " 'binder': 552,\n",
       " 'binding': 553,\n",
       " 'bingo': 554,\n",
       " 'bird': 555,\n",
       " 'birds': 556,\n",
       " 'birkenstocks': 557,\n",
       " 'birth': 558,\n",
       " 'birthday': 559,\n",
       " 'bit': 560,\n",
       " 'bite': 561,\n",
       " 'bits': 562,\n",
       " 'bitter': 563,\n",
       " 'bizarre': 564,\n",
       " 'bl': 565,\n",
       " 'black': 566,\n",
       " 'black-and': 567,\n",
       " 'blacks': 568,\n",
       " 'blades': 569,\n",
       " 'blah': 570,\n",
       " 'bland': 571,\n",
       " 'blank': 572,\n",
       " 'blanket': 573,\n",
       " 'blankets': 574,\n",
       " 'blazer': 575,\n",
       " 'blazers': 576,\n",
       " 'blazing': 577,\n",
       " 'bleach': 578,\n",
       " 'bleached': 579,\n",
       " 'bled': 580,\n",
       " 'bleed': 581,\n",
       " 'blend': 582,\n",
       " 'blends': 583,\n",
       " 'blessed': 584,\n",
       " 'blight': 585,\n",
       " 'blindly': 586,\n",
       " 'bling': 587,\n",
       " 'blk': 588,\n",
       " 'bloated': 589,\n",
       " 'blob': 590,\n",
       " 'block': 591,\n",
       " 'blocked': 592,\n",
       " 'blocking': 593,\n",
       " 'blocks': 594,\n",
       " 'blog': 595,\n",
       " 'blogger': 596,\n",
       " 'blond': 597,\n",
       " 'blonde': 598,\n",
       " 'blood': 599,\n",
       " 'bloom': 600,\n",
       " 'blossoms': 601,\n",
       " 'blotchy': 602,\n",
       " 'blouse': 603,\n",
       " 'blouse-y': 604,\n",
       " 'bloused': 605,\n",
       " 'blouses': 606,\n",
       " 'blousey': 607,\n",
       " 'blousing': 608,\n",
       " 'blouson': 609,\n",
       " 'blousy': 610,\n",
       " 'blow': 611,\n",
       " 'blown': 612,\n",
       " 'blows': 613,\n",
       " 'blowzy': 614,\n",
       " 'blu': 615,\n",
       " 'blue': 616,\n",
       " 'blue-green': 617,\n",
       " 'blue-grey': 618,\n",
       " 'blue-ish': 619,\n",
       " 'blueish': 620,\n",
       " 'blues': 621,\n",
       " 'bluish': 622,\n",
       " 'bluishgreen': 623,\n",
       " 'blush': 624,\n",
       " 'bo': 625,\n",
       " 'board': 626,\n",
       " 'boarder': 627,\n",
       " 'boardwalk': 628,\n",
       " 'boat': 629,\n",
       " 'boat-neck': 630,\n",
       " 'boatneck': 631,\n",
       " 'bod': 632,\n",
       " 'bodice': 633,\n",
       " 'bodies': 634,\n",
       " 'body': 635,\n",
       " 'body-hugging': 636,\n",
       " 'body-skimming': 637,\n",
       " 'body-type': 638,\n",
       " 'bodycon': 639,\n",
       " 'bodysuit': 640,\n",
       " 'bodytype': 641,\n",
       " 'bog': 642,\n",
       " 'bohemian': 643,\n",
       " 'boho': 644,\n",
       " 'boho-chic': 645,\n",
       " 'boiled': 646,\n",
       " 'bold': 647,\n",
       " 'bolder': 648,\n",
       " 'bolero': 649,\n",
       " 'bomb': 650,\n",
       " 'bomber': 651,\n",
       " 'bomber-style': 652,\n",
       " 'bone': 653,\n",
       " 'boned': 654,\n",
       " 'bones': 655,\n",
       " 'boning': 656,\n",
       " 'bonnet': 657,\n",
       " 'bonus': 658,\n",
       " 'boo': 659,\n",
       " 'boob': 660,\n",
       " 'boobs': 661,\n",
       " 'book': 662,\n",
       " 'books': 663,\n",
       " 'boost': 664,\n",
       " 'boot': 665,\n",
       " 'bootcut': 666,\n",
       " 'bootie': 667,\n",
       " 'booties': 668,\n",
       " 'boots': 669,\n",
       " 'booty': 670,\n",
       " 'bordeaux': 671,\n",
       " 'border': 672,\n",
       " 'bordered': 673,\n",
       " 'borderline': 674,\n",
       " 'borders': 675,\n",
       " 'boring': 676,\n",
       " 'born': 677,\n",
       " 'borrow': 678,\n",
       " 'borrowed': 679,\n",
       " 'bosom': 680,\n",
       " 'boston': 681,\n",
       " 'bot': 682,\n",
       " 'bother': 683,\n",
       " 'bothered': 684,\n",
       " 'bothers': 685,\n",
       " 'bothersome': 686,\n",
       " 'bottom': 687,\n",
       " 'bottom-heavy': 688,\n",
       " 'bottoms': 689,\n",
       " 'bottons': 690,\n",
       " 'boucle': 691,\n",
       " 'bough': 692,\n",
       " 'bounce': 693,\n",
       " 'bouncy': 694,\n",
       " 'bow': 695,\n",
       " 'bows': 696,\n",
       " 'box': 697,\n",
       " 'boxes': 698,\n",
       " 'boxier': 699,\n",
       " 'boxiness': 700,\n",
       " 'boxing': 701,\n",
       " 'boxy': 702,\n",
       " 'boy': 703,\n",
       " 'boyfriend': 704,\n",
       " 'boyfriends': 705,\n",
       " 'boyish': 706,\n",
       " 'boyleg': 707,\n",
       " 'boys': 708,\n",
       " 'br': 709,\n",
       " 'bra': 710,\n",
       " 'bra-less': 711,\n",
       " 'bra-straps': 712,\n",
       " 'bracelet': 713,\n",
       " 'braided': 714,\n",
       " 'brainer': 715,\n",
       " 'braless': 716,\n",
       " 'bralette': 717,\n",
       " 'bralettes': 718,\n",
       " 'branch': 719,\n",
       " 'branches': 720,\n",
       " 'brand': 721,\n",
       " 'brands': 722,\n",
       " 'bras': 723,\n",
       " 'brass': 724,\n",
       " 'break': 725,\n",
       " 'breaker': 726,\n",
       " 'breakers': 727,\n",
       " 'breaking': 728,\n",
       " 'breaks': 729,\n",
       " 'breast': 730,\n",
       " 'breasted': 731,\n",
       " 'breastfeeding': 732,\n",
       " 'breasting': 733,\n",
       " 'breasts': 734,\n",
       " 'breath': 735,\n",
       " 'breathable': 736,\n",
       " 'breathe': 737,\n",
       " 'breathes': 738,\n",
       " 'breathing': 739,\n",
       " 'breathtaking': 740,\n",
       " 'breeze': 741,\n",
       " 'breezy': 742,\n",
       " 'breton': 743,\n",
       " 'brick': 744,\n",
       " 'bridal': 745,\n",
       " 'bride': 746,\n",
       " 'bridesmaid': 747,\n",
       " 'bridesmaids': 748,\n",
       " 'briefly': 749,\n",
       " 'briefs': 750,\n",
       " 'bright': 751,\n",
       " 'brighten': 752,\n",
       " 'brightens': 753,\n",
       " 'brighter': 754,\n",
       " 'brightness': 755,\n",
       " 'brilliant': 756,\n",
       " 'bring': 757,\n",
       " 'bringing': 758,\n",
       " 'brings': 759,\n",
       " 'brioche': 760,\n",
       " 'brisk': 761,\n",
       " 'british': 762,\n",
       " 'broach': 763,\n",
       " 'broad': 764,\n",
       " 'broad-shouldered': 765,\n",
       " 'broaden': 766,\n",
       " 'broadened': 767,\n",
       " 'broader': 768,\n",
       " 'brocade': 769,\n",
       " 'broke': 770,\n",
       " 'broken': 771,\n",
       " 'bronze': 772,\n",
       " 'brooklyn': 773,\n",
       " \"brother's\": 774,\n",
       " 'brought': 775,\n",
       " 'brown': 776,\n",
       " 'brownish': 777,\n",
       " 'browns': 778,\n",
       " 'browse': 779,\n",
       " 'browsing': 780,\n",
       " 'brunch': 781,\n",
       " 'brunette': 782,\n",
       " 'brush': 783,\n",
       " 'brushed': 784,\n",
       " 'brushing': 785,\n",
       " 'bs': 786,\n",
       " 'btu': 787,\n",
       " 'btw': 788,\n",
       " 'bu': 789,\n",
       " 'bubble': 790,\n",
       " 'bubble-like': 791,\n",
       " 'bubbled': 792,\n",
       " 'bubbles': 793,\n",
       " 'buckle': 794,\n",
       " 'buckled': 795,\n",
       " 'buckles': 796,\n",
       " 'bucks': 797,\n",
       " 'budge': 798,\n",
       " 'budget': 799,\n",
       " 'bug': 800,\n",
       " 'bugs': 801,\n",
       " 'build': 802,\n",
       " 'building': 803,\n",
       " 'builds': 804,\n",
       " 'built': 805,\n",
       " 'built-in': 806,\n",
       " 'bulge': 807,\n",
       " 'bulged': 808,\n",
       " 'bulges': 809,\n",
       " 'bulging': 810,\n",
       " 'bulk': 811,\n",
       " 'bulkier': 812,\n",
       " 'bulkiness': 813,\n",
       " 'bulky': 814,\n",
       " 'bullet': 815,\n",
       " 'bum': 816,\n",
       " 'bummed': 817,\n",
       " 'bummer': 818,\n",
       " 'bump': 819,\n",
       " 'bumped': 820,\n",
       " 'bumps': 821,\n",
       " 'bumpy': 822,\n",
       " 'bun': 823,\n",
       " 'bunch': 824,\n",
       " 'bunched': 825,\n",
       " 'bunches': 826,\n",
       " 'bunching': 827,\n",
       " 'bunchy': 828,\n",
       " 'bundle': 829,\n",
       " 'burgers': 830,\n",
       " 'burgundy': 831,\n",
       " 'burlap': 832,\n",
       " 'burn': 833,\n",
       " 'burned': 834,\n",
       " 'burnout': 835,\n",
       " 'burns': 836,\n",
       " 'burnt': 837,\n",
       " 'burst': 838,\n",
       " 'bus': 839,\n",
       " 'business': 840,\n",
       " 'business-casual': 841,\n",
       " 'bust': 842,\n",
       " 'bust-line': 843,\n",
       " 'busted': 844,\n",
       " 'bustier': 845,\n",
       " 'busting': 846,\n",
       " 'bustline': 847,\n",
       " 'busts': 848,\n",
       " 'busty': 849,\n",
       " 'busy': 850,\n",
       " 'butt': 851,\n",
       " 'butter': 852,\n",
       " 'butterflies': 853,\n",
       " 'butterfly': 854,\n",
       " 'buttery': 855,\n",
       " 'button': 856,\n",
       " 'button-down': 857,\n",
       " 'button-front': 858,\n",
       " 'button-up': 859,\n",
       " 'buttondown': 860,\n",
       " 'buttoned': 861,\n",
       " 'buttoned-up': 862,\n",
       " 'buttonhole': 863,\n",
       " 'buttonholes': 864,\n",
       " 'buttoning': 865,\n",
       " 'buttons': 866,\n",
       " 'butts': 867,\n",
       " 'buy': 868,\n",
       " 'buyer': 869,\n",
       " 'buyers': 870,\n",
       " 'buyi': 871,\n",
       " 'buying': 872,\n",
       " 'byron': 873,\n",
       " 'c-cup': 874,\n",
       " 'c-d': 875,\n",
       " 'ca': 876,\n",
       " 'cable': 877,\n",
       " 'cacti': 878,\n",
       " 'cafe': 879,\n",
       " 'caftan': 880,\n",
       " 'cage': 881,\n",
       " 'cake': 882,\n",
       " 'cal': 883,\n",
       " 'calf': 884,\n",
       " 'cali': 885,\n",
       " 'california': 886,\n",
       " 'call': 887,\n",
       " 'called': 888,\n",
       " 'calling': 889,\n",
       " 'calls': 890,\n",
       " 'calves': 891,\n",
       " 'cam': 892,\n",
       " 'camel': 893,\n",
       " 'camera': 894,\n",
       " 'cami': 895,\n",
       " \"cami's\": 896,\n",
       " 'camis': 897,\n",
       " 'camisol': 898,\n",
       " 'camisole': 899,\n",
       " 'camisoles': 900,\n",
       " 'camo': 901,\n",
       " 'camouflage': 902,\n",
       " 'camouflages': 903,\n",
       " 'camouflaging': 904,\n",
       " 'camp': 905,\n",
       " 'campus': 906,\n",
       " 'cancel': 907,\n",
       " 'cancelled': 908,\n",
       " 'cancer': 909,\n",
       " 'canvas': 910,\n",
       " 'canvas-y': 911,\n",
       " 'cap': 912,\n",
       " 'cape': 913,\n",
       " 'capes': 914,\n",
       " 'capped': 915,\n",
       " 'capri': 916,\n",
       " 'capris': 917,\n",
       " 'capsule': 918,\n",
       " 'capture': 919,\n",
       " 'car': 920,\n",
       " 'caramel': 921,\n",
       " 'carbon': 922,\n",
       " 'card': 923,\n",
       " 'cardi': 924,\n",
       " 'cardigan': 925,\n",
       " 'cardigans': 926,\n",
       " 'cardio': 927,\n",
       " 'cardis': 928,\n",
       " 'care': 929,\n",
       " 'carefree': 930,\n",
       " 'careful': 931,\n",
       " 'carefully': 932,\n",
       " 'cares': 933,\n",
       " 'cargo': 934,\n",
       " 'cargos': 935,\n",
       " 'caribbean': 936,\n",
       " 'carissima': 937,\n",
       " 'carpet': 938,\n",
       " 'carried': 939,\n",
       " 'carries': 940,\n",
       " 'carry': 941,\n",
       " 'carrying': 942,\n",
       " 'cart': 943,\n",
       " 'cartonnier': 944,\n",
       " 'cartoon': 945,\n",
       " 'cartoonish': 946,\n",
       " 'cas': 947,\n",
       " 'cascade': 948,\n",
       " 'cascades': 949,\n",
       " 'case': 950,\n",
       " 'cases': 951,\n",
       " 'cash': 952,\n",
       " 'cashier': 953,\n",
       " 'cashmere': 954,\n",
       " 'casing': 955,\n",
       " 'casu': 956,\n",
       " 'casual': 957,\n",
       " 'casually': 958,\n",
       " 'cat': 959,\n",
       " 'catalog': 960,\n",
       " 'catalogue': 961,\n",
       " 'catch': 962,\n",
       " 'catcher': 963,\n",
       " 'catches': 964,\n",
       " 'catching': 965,\n",
       " 'catchy': 966,\n",
       " 'categorized': 967,\n",
       " 'category': 968,\n",
       " 'cats': 969,\n",
       " 'caught': 970,\n",
       " 'causal': 971,\n",
       " 'caused': 972,\n",
       " 'causing': 973,\n",
       " 'caution': 974,\n",
       " 'cautious': 975,\n",
       " 'cave': 976,\n",
       " 'caveat': 977,\n",
       " 'caved': 978,\n",
       " 'cc': 979,\n",
       " 'ceases': 980,\n",
       " 'cedar': 981,\n",
       " 'cehst': 982,\n",
       " 'celadon': 983,\n",
       " 'celebrate': 984,\n",
       " 'celebration': 985,\n",
       " 'cell': 986,\n",
       " 'cellophane': 987,\n",
       " 'cellulite': 988,\n",
       " 'center': 989,\n",
       " 'centered': 990,\n",
       " 'cents': 991,\n",
       " 'ceremony': 992,\n",
       " 'ch': 993,\n",
       " 'chain': 994,\n",
       " 'chair': 995,\n",
       " 'challenge': 996,\n",
       " 'challenged': 997,\n",
       " 'challenges': 998,\n",
       " 'challenging': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 47\n",
    "cVectorizer = CountVectorizer(analyzer = \"word\",vocabulary = vocab) # initialised the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 48\n",
    "count_features = cVectorizer.fit_transform([' '.join(review) for review in review_text]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19652, 7529)\n"
     ]
    }
   ],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 48\n",
    "print(count_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 687)\t1\n",
      "  (0, 1028)\t1\n",
      "  (0, 1716)\t1\n",
      "  (0, 1792)\t1\n",
      "  (0, 2289)\t1\n",
      "  (0, 2481)\t1\n",
      "  (0, 2602)\t1\n",
      "  (0, 2892)\t2\n",
      "  (0, 3010)\t1\n",
      "  (0, 3087)\t1\n",
      "  (0, 3193)\t1\n",
      "  (0, 3258)\t1\n",
      "  (0, 3549)\t2\n",
      "  (0, 3552)\t1\n",
      "  (0, 3832)\t1\n",
      "  (0, 3934)\t1\n",
      "  (0, 4224)\t2\n",
      "  (0, 4234)\t1\n",
      "  (0, 4427)\t1\n",
      "  (0, 4639)\t2\n",
      "  (0, 5260)\t1\n",
      "  (0, 5668)\t1\n",
      "  (0, 6726)\t1\n",
      "  (0, 7092)\t1\n",
      "  (0, 7207)\t1\n",
      "  :\t:\n",
      "  (19650, 2376)\t1\n",
      "  (19650, 2602)\t1\n",
      "  (19650, 3707)\t1\n",
      "  (19650, 3934)\t1\n",
      "  (19650, 4096)\t1\n",
      "  (19650, 4268)\t1\n",
      "  (19650, 4277)\t1\n",
      "  (19650, 4621)\t1\n",
      "  (19650, 5782)\t1\n",
      "  (19650, 5812)\t1\n",
      "  (19650, 6108)\t1\n",
      "  (19650, 6413)\t1\n",
      "  (19650, 6534)\t1\n",
      "  (19650, 6539)\t1\n",
      "  (19650, 7190)\t1\n",
      "  (19650, 7207)\t1\n",
      "  (19650, 7275)\t1\n",
      "  (19650, 7428)\t1\n",
      "  (19651, 1246)\t1\n",
      "  (19651, 2020)\t1\n",
      "  (19651, 2382)\t1\n",
      "  (19651, 3026)\t1\n",
      "  (19651, 3761)\t1\n",
      "  (19651, 4621)\t1\n",
      "  (19651, 5153)\t1\n"
     ]
    }
   ],
   "source": [
    "print(count_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representation as per spectification.\n",
    "- count_vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the count vector representation in a file in '#index,word_integer_index:word_freq' format\n",
    "with open(\"count_vectors.txt\", \"w\") as count_file: # opening the file in write mode\n",
    "    for i, review in enumerate(count_features): # looping thorough each review and its corresponding feature vector\n",
    "        descp = ','.join(f\"{idx}:{count_features[i, idx]}\" for idx in review.indices) # geting the word index and its frequency\n",
    "        count_file.write(f\"#{i},{descp}\\n\") # writing it in the said format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] w08_act2_embedding_classification - cell 11\n",
    "# loading the pre-trained glove model\n",
    "preTW2v_wv = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates document vectors by summing the embeddings of valid words for each document. The dimensions of the matrix are set such that number of rows is number of documents and number of columns is corresponding embedding dimensions. Each document is represented as a single vector, which is the sum of the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] w08_act2_embedding_classification - cell 15\n",
    "# function that generates document vectors by summing the embeddings of valid words\n",
    "def docvecs(embeddings, docs):\n",
    "    vecs = np.zeros((len(docs), embeddings.vector_size)) # initialising empty matrix \n",
    "    for i, doc in enumerate(docs): # looping through each document and its index\n",
    "        valid_keys = [term for term in doc if term in embeddings.key_to_index] # filtering words that exist in embeddings vocabulary\n",
    "        docvec = np.vstack([embeddings[term] for term in valid_keys]) # stacking the words and creating a matrix\n",
    "        docvec = np.sum(docvec, axis=0) # summing the embeddings along the first axis to get a single vector representing the document\n",
    "        vecs[i,:] = docvec # assigning the computed document vector to the corresponding row in the matrix\n",
    "    return vecs # returning the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] w08_act2_embedding_classification - cell 16\n",
    "unweighted_vec_rep = docvecs(preTW2v_wv, review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.57157302, -3.96359205,  3.31884074, ..., -0.23908034,\n",
       "         2.97417307, -2.44539618],\n",
       "       [-2.34051514,  1.39787209, -4.70805264, ..., -1.65546536,\n",
       "         0.38116995,  1.65550005],\n",
       "       [-5.53618002,  3.77994132, -2.72567987, ..., -6.70081997,\n",
       "        -1.06155181, -4.32599688],\n",
       "       ...,\n",
       "       [ 1.12529898, -0.69280398,  0.26605999, ..., -1.41801202,\n",
       "        -0.11023009,  2.23825979],\n",
       "       [ 2.91901064,  3.47800589,  1.67871833, ..., -4.15545177,\n",
       "        -2.15171194,  2.47598886],\n",
       "       [ 1.30052102,  0.42059994, -5.66381025, ..., -0.34029651,\n",
       "         1.61652899,  3.62967396]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [2] w08_act2_embedding_classification\n",
    "unweighted_vec_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19652, 7529)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 51\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tVectorizer = TfidfVectorizer(analyzer = \"word\",vocabulary = vocab) # initialising the TfidfVectorizer\n",
    "tfidf_features = tVectorizer.fit_transform([' '.join(review) for review in review_text]) # generating the tfidf vector representation for all reviews\n",
    "tfidf_features.shape # checking the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7522)\t0.13796655643162234\n",
      "  (0, 7520)\t0.15790936302533073\n",
      "  (0, 7406)\t0.08942754431903749\n",
      "  (0, 7207)\t0.10688230992054923\n",
      "  (0, 7092)\t0.1108979967093157\n",
      "  (0, 6726)\t0.09944270371682062\n",
      "  (0, 5668)\t0.15731076193390778\n",
      "  (0, 5260)\t0.20116611478729465\n",
      "  (0, 4639)\t0.17993461419688872\n",
      "  (0, 4427)\t0.2567966207221875\n",
      "  (0, 4234)\t0.11400095142158763\n",
      "  (0, 4224)\t0.4867876227942154\n",
      "  (0, 3934)\t0.09045659947118836\n",
      "  (0, 3832)\t0.19468911285051188\n",
      "  (0, 3552)\t0.16401760637664864\n",
      "  (0, 3549)\t0.2738967869996706\n",
      "  (0, 3258)\t0.17097793161240013\n",
      "  (0, 3193)\t0.19309847172209627\n",
      "  (0, 3087)\t0.16844705121993458\n",
      "  (0, 3010)\t0.10509713403953778\n",
      "  (0, 2892)\t0.29992142257858656\n",
      "  (0, 2602)\t0.11016820762903567\n",
      "  (0, 2481)\t0.1869368578273477\n",
      "  (0, 2289)\t0.14687037381367216\n",
      "  (0, 1792)\t0.22042366265962396\n",
      "  :\t:\n",
      "  (19650, 6539)\t0.2395403434353545\n",
      "  (19650, 6534)\t0.23141827664279\n",
      "  (19650, 6413)\t0.14614087003193094\n",
      "  (19650, 6108)\t0.2720204291731219\n",
      "  (19650, 5812)\t0.16508223856582344\n",
      "  (19650, 5782)\t0.16294897935805366\n",
      "  (19650, 4621)\t0.14685350204204836\n",
      "  (19650, 4277)\t0.24159956722480888\n",
      "  (19650, 4268)\t0.3828450218152042\n",
      "  (19650, 4096)\t0.23920743743746067\n",
      "  (19650, 3934)\t0.13485714376024166\n",
      "  (19650, 3707)\t0.12889314891432355\n",
      "  (19650, 2602)\t0.16424417788078757\n",
      "  (19650, 2376)\t0.17246391318513749\n",
      "  (19650, 1678)\t0.3198898965323079\n",
      "  (19650, 1030)\t0.30490868785262926\n",
      "  (19650, 842)\t0.164341846392573\n",
      "  (19650, 537)\t0.13996223076499517\n",
      "  (19651, 5153)\t0.3535381969149261\n",
      "  (19651, 4621)\t0.3238746550168572\n",
      "  (19651, 3761)\t0.3530532717566595\n",
      "  (19651, 3026)\t0.4299035987090085\n",
      "  (19651, 2382)\t0.4172135715031913\n",
      "  (19651, 2020)\t0.3840165341051054\n",
      "  (19651, 1246)\t0.3729830714205012\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_features) # checking the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function writes the TF-IDF vector representation of each document into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] Act 3_ Generating Feature Vectors.ipynb - cell 58\n",
    "def write_vectorFile(tfidf_features,filename):\n",
    "    num = tfidf_features.shape[0] # getting the number of documents\n",
    "    out_file = open(filename, 'w') # opening the file in write mode\n",
    "    for a_ind in range(0, num): # loop through each review \n",
    "        for f_ind in tfidf_features[a_ind].nonzero()[1]: # looping through non-zero elements\n",
    "            value = tfidf_features[a_ind][0,f_ind] # retrieving the value of the entry \n",
    "            out_file.write(\"{}:{} \".format(f_ind,value)) # writing in the said format\n",
    "        out_file.write('\\n') # new line\n",
    "    out_file.close() # closing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] Act 3_ Generating Feature Vectors.ipynb - cell 59\n",
    "write_vectorFile(tfidf_features,\"./reviews_tVector.txt\") # writing the tfidf vector to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a-cup': 0,\n",
       " 'a-flutter': 1,\n",
       " 'a-frame': 2,\n",
       " 'a-kind': 3,\n",
       " 'a-line': 4,\n",
       " 'a-lines': 5,\n",
       " 'a-symmetric': 6,\n",
       " 'aa': 7,\n",
       " 'ab': 8,\n",
       " 'abbey': 9,\n",
       " 'abby': 10,\n",
       " 'abdomen': 11,\n",
       " 'ability': 12,\n",
       " 'abnormally': 13,\n",
       " 'abo': 14,\n",
       " 'abou': 15,\n",
       " 'above-the': 16,\n",
       " 'abroad': 17,\n",
       " 'abs': 18,\n",
       " 'absolute': 19,\n",
       " 'absolutely': 20,\n",
       " 'absolutley': 21,\n",
       " 'absolutly': 22,\n",
       " 'abstract': 23,\n",
       " 'absurd': 24,\n",
       " 'abt': 25,\n",
       " 'abundance': 26,\n",
       " 'ac': 27,\n",
       " 'accent': 28,\n",
       " 'accented': 29,\n",
       " 'accenting': 30,\n",
       " 'accents': 31,\n",
       " 'accentuate': 32,\n",
       " 'accentuated': 33,\n",
       " 'accentuates': 34,\n",
       " 'accentuating': 35,\n",
       " 'accept': 36,\n",
       " 'acceptable': 37,\n",
       " 'accepted': 38,\n",
       " 'access': 39,\n",
       " 'accessories': 40,\n",
       " 'accessorize': 41,\n",
       " 'accessorized': 42,\n",
       " 'accessorizing': 43,\n",
       " 'accessory': 44,\n",
       " 'accident': 45,\n",
       " 'accidental': 46,\n",
       " 'accidentally': 47,\n",
       " 'accommodate': 48,\n",
       " 'accommodated': 49,\n",
       " 'accommodates': 50,\n",
       " 'accommodating': 51,\n",
       " 'accomodate': 52,\n",
       " 'accompanying': 53,\n",
       " 'accomplish': 54,\n",
       " 'accordian': 55,\n",
       " 'account': 56,\n",
       " 'accurate': 57,\n",
       " 'accurately': 58,\n",
       " 'acetate': 59,\n",
       " 'achieve': 60,\n",
       " 'acrylic': 61,\n",
       " 'act': 62,\n",
       " 'action': 63,\n",
       " 'active': 64,\n",
       " 'activewear': 65,\n",
       " 'activities': 66,\n",
       " 'acts': 67,\n",
       " 'actual': 68,\n",
       " 'actuality': 69,\n",
       " 'ad': 70,\n",
       " 'ada': 71,\n",
       " 'add': 72,\n",
       " 'add-on': 73,\n",
       " 'added': 74,\n",
       " 'addict': 75,\n",
       " 'addicted': 76,\n",
       " 'adding': 77,\n",
       " 'addition': 78,\n",
       " 'additional': 79,\n",
       " 'additionally': 80,\n",
       " 'address': 81,\n",
       " 'adds': 82,\n",
       " 'adequate': 83,\n",
       " 'adequately': 84,\n",
       " 'adjust': 85,\n",
       " 'adjustable': 86,\n",
       " 'adjusted': 87,\n",
       " 'adjusting': 88,\n",
       " 'adjustment': 89,\n",
       " 'adjustments': 90,\n",
       " 'admire': 91,\n",
       " 'admired': 92,\n",
       " 'admiring': 93,\n",
       " 'admit': 94,\n",
       " 'admittedly': 95,\n",
       " 'adn': 96,\n",
       " 'ador': 97,\n",
       " 'adorable': 98,\n",
       " 'adore': 99,\n",
       " 'adored': 100,\n",
       " 'ads': 101,\n",
       " 'adult': 102,\n",
       " 'adults': 103,\n",
       " 'advantage': 104,\n",
       " 'advantages': 105,\n",
       " 'adventure': 106,\n",
       " 'advertised': 107,\n",
       " 'advice': 108,\n",
       " 'advise': 109,\n",
       " 'advised': 110,\n",
       " 'aesthetic': 111,\n",
       " 'aesthetically': 112,\n",
       " 'aesthetics': 113,\n",
       " 'affair': 114,\n",
       " 'affect': 115,\n",
       " 'affected': 116,\n",
       " 'affects': 117,\n",
       " 'afford': 118,\n",
       " 'affordable': 119,\n",
       " 'aforementioned': 120,\n",
       " 'afraid': 121,\n",
       " 'afternoon': 122,\n",
       " 'afterward': 123,\n",
       " 'ag': 124,\n",
       " \"ag's\": 125,\n",
       " 'age': 126,\n",
       " 'age-appropriate': 127,\n",
       " 'aged': 128,\n",
       " 'ages': 129,\n",
       " 'aggressive': 130,\n",
       " 'agin': 131,\n",
       " 'ago': 132,\n",
       " 'agree': 133,\n",
       " 'agreed': 134,\n",
       " 'agreement': 135,\n",
       " 'ags': 136,\n",
       " 'ah': 137,\n",
       " 'ahd': 138,\n",
       " 'ahead': 139,\n",
       " 'ahhh': 140,\n",
       " 'ahs': 141,\n",
       " 'ahve': 142,\n",
       " 'air': 143,\n",
       " 'air-dried': 144,\n",
       " 'air-drying': 145,\n",
       " 'airiness': 146,\n",
       " 'airing': 147,\n",
       " 'airplanes': 148,\n",
       " 'airport': 149,\n",
       " 'airy': 150,\n",
       " 'aize': 151,\n",
       " 'aka': 152,\n",
       " 'akemi': 153,\n",
       " 'al': 154,\n",
       " 'alas': 155,\n",
       " 'alaska': 156,\n",
       " 'albeit': 157,\n",
       " 'alexandria': 158,\n",
       " 'align': 159,\n",
       " 'aligned': 160,\n",
       " 'alike': 161,\n",
       " 'aline': 162,\n",
       " 'alittle': 163,\n",
       " 'all-around': 164,\n",
       " 'all-in': 165,\n",
       " 'all-over': 166,\n",
       " 'alley': 167,\n",
       " 'allison': 168,\n",
       " 'allover': 169,\n",
       " 'allowed': 170,\n",
       " 'allowing': 171,\n",
       " 'allusion': 172,\n",
       " 'alot': 173,\n",
       " 'alpaca': 174,\n",
       " 'alright': 175,\n",
       " 'als': 176,\n",
       " 'alter': 177,\n",
       " 'alteration': 178,\n",
       " 'alterations': 179,\n",
       " 'altered': 180,\n",
       " 'altering': 181,\n",
       " 'alternate': 182,\n",
       " 'alternations': 183,\n",
       " 'alternative': 184,\n",
       " 'altho': 185,\n",
       " 'altogether': 186,\n",
       " 'amadi': 187,\n",
       " 'amalfi': 188,\n",
       " 'amazed': 189,\n",
       " 'amazing': 190,\n",
       " 'amazingly': 191,\n",
       " 'amazon': 192,\n",
       " 'ambiguous': 193,\n",
       " 'amd': 194,\n",
       " 'american': 195,\n",
       " 'amount': 196,\n",
       " 'amounts': 197,\n",
       " 'amp': 198,\n",
       " 'ample': 199,\n",
       " 'amply': 200,\n",
       " 'amterial': 201,\n",
       " 'anatomy': 202,\n",
       " 'and-go': 203,\n",
       " 'angel': 204,\n",
       " 'angeles': 205,\n",
       " 'angle': 206,\n",
       " 'angled': 207,\n",
       " 'angles': 208,\n",
       " 'angora': 209,\n",
       " 'animal': 210,\n",
       " 'animals': 211,\n",
       " 'ankle': 212,\n",
       " 'ankle-length': 213,\n",
       " 'ankles': 214,\n",
       " \"ann's\": 215,\n",
       " 'anna': 216,\n",
       " 'anniversary': 217,\n",
       " 'annoyance': 218,\n",
       " 'annoyed': 219,\n",
       " 'annoying': 220,\n",
       " 'anorak': 221,\n",
       " 'ans': 222,\n",
       " 'answer': 223,\n",
       " 'ant': 224,\n",
       " 'anth': 225,\n",
       " 'anther': 226,\n",
       " 'antho': 227,\n",
       " 'anticipate': 228,\n",
       " 'anticipated': 229,\n",
       " 'anticipating': 230,\n",
       " 'anticipation': 231,\n",
       " 'antique': 232,\n",
       " 'antrho': 233,\n",
       " 'antro': 234,\n",
       " 'anxious': 235,\n",
       " 'anxiously': 236,\n",
       " 'anymore': 237,\n",
       " \"anyone's\": 238,\n",
       " 'anytime': 239,\n",
       " 'app': 240,\n",
       " 'appalled': 241,\n",
       " 'apparel': 242,\n",
       " 'apparent': 243,\n",
       " 'apparently': 244,\n",
       " 'appeal': 245,\n",
       " 'appealed': 246,\n",
       " 'appealing': 247,\n",
       " 'appearance': 248,\n",
       " 'appeared': 249,\n",
       " 'appearing': 250,\n",
       " 'appears': 251,\n",
       " 'apple': 252,\n",
       " 'apple-shaped': 253,\n",
       " 'applied': 254,\n",
       " 'appliqu': 255,\n",
       " 'applique': 256,\n",
       " 'apply': 257,\n",
       " 'appreciated': 258,\n",
       " 'apprehensive': 259,\n",
       " 'approach': 260,\n",
       " 'approaching': 261,\n",
       " 'appropriately': 262,\n",
       " 'approved': 263,\n",
       " 'approx': 264,\n",
       " 'approximate': 265,\n",
       " 'approximately': 266,\n",
       " 'apprx': 267,\n",
       " 'apricot': 268,\n",
       " 'april': 269,\n",
       " 'apron': 270,\n",
       " 'apt': 271,\n",
       " 'aqua': 272,\n",
       " 'ar': 273,\n",
       " 'arc': 274,\n",
       " 'area': 275,\n",
       " 'areas': 276,\n",
       " 'aren': 277,\n",
       " 'arielle': 278,\n",
       " 'arm': 279,\n",
       " 'arm-holes': 280,\n",
       " 'armed': 281,\n",
       " 'armhole': 282,\n",
       " 'armholes': 283,\n",
       " 'armpit': 284,\n",
       " 'armpits': 285,\n",
       " 'arms': 286,\n",
       " 'army': 287,\n",
       " 'arranged': 288,\n",
       " 'arrival': 289,\n",
       " 'arrivals': 290,\n",
       " 'arrive': 291,\n",
       " 'arrived': 292,\n",
       " 'arrives': 293,\n",
       " 'arrows': 294,\n",
       " 'art': 295,\n",
       " 'article': 296,\n",
       " 'articles': 297,\n",
       " 'artist': 298,\n",
       " \"artist's\": 299,\n",
       " 'artistic': 300,\n",
       " 'artsy': 301,\n",
       " 'artwork': 302,\n",
       " 'arty-looking': 303,\n",
       " 'as-is': 304,\n",
       " 'as-pictured': 305,\n",
       " 'asap': 306,\n",
       " 'ashley': 307,\n",
       " 'asia': 308,\n",
       " 'asked': 309,\n",
       " 'aspect': 310,\n",
       " 'aspects': 311,\n",
       " 'assessing': 312,\n",
       " 'assets': 313,\n",
       " 'assistance': 314,\n",
       " 'assistant': 315,\n",
       " 'associate': 316,\n",
       " 'associates': 317,\n",
       " 'assume': 318,\n",
       " 'assumed': 319,\n",
       " 'assuming': 320,\n",
       " 'assumption': 321,\n",
       " 'assured': 322,\n",
       " 'astounded': 323,\n",
       " 'asymmetric': 324,\n",
       " 'asymmetrical': 325,\n",
       " 'asymmetry': 326,\n",
       " 'ate': 327,\n",
       " 'athleisure': 328,\n",
       " 'athlete': 329,\n",
       " 'athletic': 330,\n",
       " 'atl': 331,\n",
       " 'atlanta': 332,\n",
       " 'atleast': 333,\n",
       " 'atrocious': 334,\n",
       " 'attach': 335,\n",
       " 'attached': 336,\n",
       " 'attaches': 337,\n",
       " 'attachment': 338,\n",
       " 'attempt': 339,\n",
       " 'attempted': 340,\n",
       " 'attempting': 341,\n",
       " 'attend': 342,\n",
       " 'attendant': 343,\n",
       " 'attended': 344,\n",
       " 'attending': 345,\n",
       " 'attention': 346,\n",
       " 'attire': 347,\n",
       " 'attitude': 348,\n",
       " 'attracted': 349,\n",
       " 'attracting': 350,\n",
       " 'attractive': 351,\n",
       " 'audrey': 352,\n",
       " 'august': 353,\n",
       " 'aunt': 354,\n",
       " 'australian': 355,\n",
       " 'authentic': 356,\n",
       " 'automatically': 357,\n",
       " 'autumn': 358,\n",
       " 'autumnal': 359,\n",
       " 'avail': 360,\n",
       " 'availability': 361,\n",
       " 'average': 362,\n",
       " 'avid': 363,\n",
       " 'avoid': 364,\n",
       " 'avoided': 365,\n",
       " 'avoiding': 366,\n",
       " 'avoids': 367,\n",
       " 'awaited': 368,\n",
       " 'awaiting': 369,\n",
       " 'awards': 370,\n",
       " 'aware': 371,\n",
       " 'awesome': 372,\n",
       " 'awful': 373,\n",
       " 'awhile': 374,\n",
       " 'awkward': 375,\n",
       " 'awkwardly': 376,\n",
       " 'awkwardness': 377,\n",
       " 'az': 378,\n",
       " 'b-c': 379,\n",
       " 'ba': 380,\n",
       " 'babies': 381,\n",
       " 'baby': 382,\n",
       " 'baby-doll': 383,\n",
       " 'babydoll': 384,\n",
       " 'bac': 385,\n",
       " 'bachelorette': 386,\n",
       " 'back-ordered': 387,\n",
       " 'back-up': 388,\n",
       " 'backed': 389,\n",
       " 'background': 390,\n",
       " 'backorder': 391,\n",
       " 'backordered': 392,\n",
       " 'backpack': 393,\n",
       " 'backs': 394,\n",
       " 'backside': 395,\n",
       " 'backup': 396,\n",
       " 'backward': 397,\n",
       " 'backwards': 398,\n",
       " 'backyard': 399,\n",
       " 'bad': 400,\n",
       " 'badly': 401,\n",
       " 'bag': 402,\n",
       " 'bagged': 403,\n",
       " 'baggie': 404,\n",
       " 'baggier': 405,\n",
       " 'bagginess': 406,\n",
       " 'bagging': 407,\n",
       " 'baggy': 408,\n",
       " 'bags': 409,\n",
       " 'bailey': 410,\n",
       " 'baily': 411,\n",
       " 'balance': 412,\n",
       " 'balanced': 413,\n",
       " 'balances': 414,\n",
       " 'balck': 415,\n",
       " 'balked': 416,\n",
       " 'ball': 417,\n",
       " 'ballet': 418,\n",
       " 'balloon': 419,\n",
       " 'ballooned': 420,\n",
       " 'balloons': 421,\n",
       " 'balloony': 422,\n",
       " 'balls': 423,\n",
       " 'bam': 424,\n",
       " 'band': 425,\n",
       " 'bandage': 426,\n",
       " 'bandeau': 427,\n",
       " 'banded': 428,\n",
       " 'banding': 429,\n",
       " 'bands': 430,\n",
       " 'bank': 431,\n",
       " 'bar': 432,\n",
       " 'barbecue': 433,\n",
       " 'bare': 434,\n",
       " 'barefoot': 435,\n",
       " 'barely': 436,\n",
       " 'bargain': 437,\n",
       " 'barley': 438,\n",
       " 'baroque': 439,\n",
       " 'barre': 440,\n",
       " 'barrel': 441,\n",
       " 'base': 442,\n",
       " 'baseball': 443,\n",
       " 'based': 444,\n",
       " 'basic': 445,\n",
       " 'basically': 446,\n",
       " 'basics': 447,\n",
       " 'basis': 448,\n",
       " 'basket': 449,\n",
       " 'basketball': 450,\n",
       " 'basketweave': 451,\n",
       " 'bat': 452,\n",
       " 'batch': 453,\n",
       " 'batches': 454,\n",
       " 'bath': 455,\n",
       " 'bathing': 456,\n",
       " 'bathrobe': 457,\n",
       " 'bathroom': 458,\n",
       " 'battle': 459,\n",
       " 'batwing': 460,\n",
       " 'batwings': 461,\n",
       " 'bay': 462,\n",
       " 'bbq': 463,\n",
       " 'bc': 464,\n",
       " 'bday': 465,\n",
       " 'bea': 466,\n",
       " 'beach': 467,\n",
       " 'beachy': 468,\n",
       " 'bead': 469,\n",
       " 'beaded': 470,\n",
       " 'beading': 471,\n",
       " 'beads': 472,\n",
       " 'beadwork': 473,\n",
       " 'bear': 474,\n",
       " 'beat': 475,\n",
       " 'beauties': 476,\n",
       " 'beautifu': 477,\n",
       " 'beautiful': 478,\n",
       " 'beautifully': 479,\n",
       " 'beautify': 480,\n",
       " 'beauty': 481,\n",
       " 'bec': 482,\n",
       " 'beca': 483,\n",
       " 'becasue': 484,\n",
       " 'becau': 485,\n",
       " 'becaus': 486,\n",
       " 'becuase': 487,\n",
       " 'bed': 488,\n",
       " 'bedtime': 489,\n",
       " 'bee': 490,\n",
       " 'beef': 491,\n",
       " 'bees': 492,\n",
       " 'began': 493,\n",
       " 'begging': 494,\n",
       " 'begin': 495,\n",
       " 'beginning': 496,\n",
       " 'begins': 497,\n",
       " 'behold': 498,\n",
       " 'beige': 499,\n",
       " 'bein': 500,\n",
       " 'bel': 501,\n",
       " 'bell': 502,\n",
       " 'bell-sleeve': 503,\n",
       " 'belled': 504,\n",
       " 'bellow': 505,\n",
       " 'bells': 506,\n",
       " 'belly': 507,\n",
       " 'bellybutton': 508,\n",
       " 'belonged': 509,\n",
       " 'beloved': 510,\n",
       " 'belt': 511,\n",
       " 'belted': 512,\n",
       " 'belting': 513,\n",
       " 'belts': 514,\n",
       " 'bend': 515,\n",
       " 'bending': 516,\n",
       " 'beneath': 517,\n",
       " 'benefit': 518,\n",
       " 'bent': 519,\n",
       " 'bermuda': 520,\n",
       " 'berry': 521,\n",
       " 'bes': 522,\n",
       " 'bet': 523,\n",
       " 'bette': 524,\n",
       " 'betty': 525,\n",
       " 'beware': 526,\n",
       " 'bf': 527,\n",
       " 'bff': 528,\n",
       " 'bi': 529,\n",
       " 'bias': 530,\n",
       " 'biased': 531,\n",
       " 'bib': 532,\n",
       " 'bicep': 533,\n",
       " 'biceps': 534,\n",
       " 'bicycle': 535,\n",
       " 'bid': 536,\n",
       " 'big': 537,\n",
       " 'bigger': 538,\n",
       " 'biggest': 539,\n",
       " 'biggie': 540,\n",
       " 'biggish': 541,\n",
       " 'biker': 542,\n",
       " 'bikini': 543,\n",
       " 'bikinis': 544,\n",
       " 'bill': 545,\n",
       " 'billow': 546,\n",
       " 'billowed': 547,\n",
       " 'billowing': 548,\n",
       " 'billows': 549,\n",
       " 'billowy': 550,\n",
       " 'bind': 551,\n",
       " 'binder': 552,\n",
       " 'binding': 553,\n",
       " 'bingo': 554,\n",
       " 'bird': 555,\n",
       " 'birds': 556,\n",
       " 'birkenstocks': 557,\n",
       " 'birth': 558,\n",
       " 'birthday': 559,\n",
       " 'bit': 560,\n",
       " 'bite': 561,\n",
       " 'bits': 562,\n",
       " 'bitter': 563,\n",
       " 'bizarre': 564,\n",
       " 'bl': 565,\n",
       " 'black': 566,\n",
       " 'black-and': 567,\n",
       " 'blacks': 568,\n",
       " 'blades': 569,\n",
       " 'blah': 570,\n",
       " 'bland': 571,\n",
       " 'blank': 572,\n",
       " 'blanket': 573,\n",
       " 'blankets': 574,\n",
       " 'blazer': 575,\n",
       " 'blazers': 576,\n",
       " 'blazing': 577,\n",
       " 'bleach': 578,\n",
       " 'bleached': 579,\n",
       " 'bled': 580,\n",
       " 'bleed': 581,\n",
       " 'blend': 582,\n",
       " 'blends': 583,\n",
       " 'blessed': 584,\n",
       " 'blight': 585,\n",
       " 'blindly': 586,\n",
       " 'bling': 587,\n",
       " 'blk': 588,\n",
       " 'bloated': 589,\n",
       " 'blob': 590,\n",
       " 'block': 591,\n",
       " 'blocked': 592,\n",
       " 'blocking': 593,\n",
       " 'blocks': 594,\n",
       " 'blog': 595,\n",
       " 'blogger': 596,\n",
       " 'blond': 597,\n",
       " 'blonde': 598,\n",
       " 'blood': 599,\n",
       " 'bloom': 600,\n",
       " 'blossoms': 601,\n",
       " 'blotchy': 602,\n",
       " 'blouse': 603,\n",
       " 'blouse-y': 604,\n",
       " 'bloused': 605,\n",
       " 'blouses': 606,\n",
       " 'blousey': 607,\n",
       " 'blousing': 608,\n",
       " 'blouson': 609,\n",
       " 'blousy': 610,\n",
       " 'blow': 611,\n",
       " 'blown': 612,\n",
       " 'blows': 613,\n",
       " 'blowzy': 614,\n",
       " 'blu': 615,\n",
       " 'blue': 616,\n",
       " 'blue-green': 617,\n",
       " 'blue-grey': 618,\n",
       " 'blue-ish': 619,\n",
       " 'blueish': 620,\n",
       " 'blues': 621,\n",
       " 'bluish': 622,\n",
       " 'bluishgreen': 623,\n",
       " 'blush': 624,\n",
       " 'bo': 625,\n",
       " 'board': 626,\n",
       " 'boarder': 627,\n",
       " 'boardwalk': 628,\n",
       " 'boat': 629,\n",
       " 'boat-neck': 630,\n",
       " 'boatneck': 631,\n",
       " 'bod': 632,\n",
       " 'bodice': 633,\n",
       " 'bodies': 634,\n",
       " 'body': 635,\n",
       " 'body-hugging': 636,\n",
       " 'body-skimming': 637,\n",
       " 'body-type': 638,\n",
       " 'bodycon': 639,\n",
       " 'bodysuit': 640,\n",
       " 'bodytype': 641,\n",
       " 'bog': 642,\n",
       " 'bohemian': 643,\n",
       " 'boho': 644,\n",
       " 'boho-chic': 645,\n",
       " 'boiled': 646,\n",
       " 'bold': 647,\n",
       " 'bolder': 648,\n",
       " 'bolero': 649,\n",
       " 'bomb': 650,\n",
       " 'bomber': 651,\n",
       " 'bomber-style': 652,\n",
       " 'bone': 653,\n",
       " 'boned': 654,\n",
       " 'bones': 655,\n",
       " 'boning': 656,\n",
       " 'bonnet': 657,\n",
       " 'bonus': 658,\n",
       " 'boo': 659,\n",
       " 'boob': 660,\n",
       " 'boobs': 661,\n",
       " 'book': 662,\n",
       " 'books': 663,\n",
       " 'boost': 664,\n",
       " 'boot': 665,\n",
       " 'bootcut': 666,\n",
       " 'bootie': 667,\n",
       " 'booties': 668,\n",
       " 'boots': 669,\n",
       " 'booty': 670,\n",
       " 'bordeaux': 671,\n",
       " 'border': 672,\n",
       " 'bordered': 673,\n",
       " 'borderline': 674,\n",
       " 'borders': 675,\n",
       " 'boring': 676,\n",
       " 'born': 677,\n",
       " 'borrow': 678,\n",
       " 'borrowed': 679,\n",
       " 'bosom': 680,\n",
       " 'boston': 681,\n",
       " 'bot': 682,\n",
       " 'bother': 683,\n",
       " 'bothered': 684,\n",
       " 'bothers': 685,\n",
       " 'bothersome': 686,\n",
       " 'bottom': 687,\n",
       " 'bottom-heavy': 688,\n",
       " 'bottoms': 689,\n",
       " 'bottons': 690,\n",
       " 'boucle': 691,\n",
       " 'bough': 692,\n",
       " 'bounce': 693,\n",
       " 'bouncy': 694,\n",
       " 'bow': 695,\n",
       " 'bows': 696,\n",
       " 'box': 697,\n",
       " 'boxes': 698,\n",
       " 'boxier': 699,\n",
       " 'boxiness': 700,\n",
       " 'boxing': 701,\n",
       " 'boxy': 702,\n",
       " 'boy': 703,\n",
       " 'boyfriend': 704,\n",
       " 'boyfriends': 705,\n",
       " 'boyish': 706,\n",
       " 'boyleg': 707,\n",
       " 'boys': 708,\n",
       " 'br': 709,\n",
       " 'bra': 710,\n",
       " 'bra-less': 711,\n",
       " 'bra-straps': 712,\n",
       " 'bracelet': 713,\n",
       " 'braided': 714,\n",
       " 'brainer': 715,\n",
       " 'braless': 716,\n",
       " 'bralette': 717,\n",
       " 'bralettes': 718,\n",
       " 'branch': 719,\n",
       " 'branches': 720,\n",
       " 'brand': 721,\n",
       " 'brands': 722,\n",
       " 'bras': 723,\n",
       " 'brass': 724,\n",
       " 'break': 725,\n",
       " 'breaker': 726,\n",
       " 'breakers': 727,\n",
       " 'breaking': 728,\n",
       " 'breaks': 729,\n",
       " 'breast': 730,\n",
       " 'breasted': 731,\n",
       " 'breastfeeding': 732,\n",
       " 'breasting': 733,\n",
       " 'breasts': 734,\n",
       " 'breath': 735,\n",
       " 'breathable': 736,\n",
       " 'breathe': 737,\n",
       " 'breathes': 738,\n",
       " 'breathing': 739,\n",
       " 'breathtaking': 740,\n",
       " 'breeze': 741,\n",
       " 'breezy': 742,\n",
       " 'breton': 743,\n",
       " 'brick': 744,\n",
       " 'bridal': 745,\n",
       " 'bride': 746,\n",
       " 'bridesmaid': 747,\n",
       " 'bridesmaids': 748,\n",
       " 'briefly': 749,\n",
       " 'briefs': 750,\n",
       " 'bright': 751,\n",
       " 'brighten': 752,\n",
       " 'brightens': 753,\n",
       " 'brighter': 754,\n",
       " 'brightness': 755,\n",
       " 'brilliant': 756,\n",
       " 'bring': 757,\n",
       " 'bringing': 758,\n",
       " 'brings': 759,\n",
       " 'brioche': 760,\n",
       " 'brisk': 761,\n",
       " 'british': 762,\n",
       " 'broach': 763,\n",
       " 'broad': 764,\n",
       " 'broad-shouldered': 765,\n",
       " 'broaden': 766,\n",
       " 'broadened': 767,\n",
       " 'broader': 768,\n",
       " 'brocade': 769,\n",
       " 'broke': 770,\n",
       " 'broken': 771,\n",
       " 'bronze': 772,\n",
       " 'brooklyn': 773,\n",
       " \"brother's\": 774,\n",
       " 'brought': 775,\n",
       " 'brown': 776,\n",
       " 'brownish': 777,\n",
       " 'browns': 778,\n",
       " 'browse': 779,\n",
       " 'browsing': 780,\n",
       " 'brunch': 781,\n",
       " 'brunette': 782,\n",
       " 'brush': 783,\n",
       " 'brushed': 784,\n",
       " 'brushing': 785,\n",
       " 'bs': 786,\n",
       " 'btu': 787,\n",
       " 'btw': 788,\n",
       " 'bu': 789,\n",
       " 'bubble': 790,\n",
       " 'bubble-like': 791,\n",
       " 'bubbled': 792,\n",
       " 'bubbles': 793,\n",
       " 'buckle': 794,\n",
       " 'buckled': 795,\n",
       " 'buckles': 796,\n",
       " 'bucks': 797,\n",
       " 'budge': 798,\n",
       " 'budget': 799,\n",
       " 'bug': 800,\n",
       " 'bugs': 801,\n",
       " 'build': 802,\n",
       " 'building': 803,\n",
       " 'builds': 804,\n",
       " 'built': 805,\n",
       " 'built-in': 806,\n",
       " 'bulge': 807,\n",
       " 'bulged': 808,\n",
       " 'bulges': 809,\n",
       " 'bulging': 810,\n",
       " 'bulk': 811,\n",
       " 'bulkier': 812,\n",
       " 'bulkiness': 813,\n",
       " 'bulky': 814,\n",
       " 'bullet': 815,\n",
       " 'bum': 816,\n",
       " 'bummed': 817,\n",
       " 'bummer': 818,\n",
       " 'bump': 819,\n",
       " 'bumped': 820,\n",
       " 'bumps': 821,\n",
       " 'bumpy': 822,\n",
       " 'bun': 823,\n",
       " 'bunch': 824,\n",
       " 'bunched': 825,\n",
       " 'bunches': 826,\n",
       " 'bunching': 827,\n",
       " 'bunchy': 828,\n",
       " 'bundle': 829,\n",
       " 'burgers': 830,\n",
       " 'burgundy': 831,\n",
       " 'burlap': 832,\n",
       " 'burn': 833,\n",
       " 'burned': 834,\n",
       " 'burnout': 835,\n",
       " 'burns': 836,\n",
       " 'burnt': 837,\n",
       " 'burst': 838,\n",
       " 'bus': 839,\n",
       " 'business': 840,\n",
       " 'business-casual': 841,\n",
       " 'bust': 842,\n",
       " 'bust-line': 843,\n",
       " 'busted': 844,\n",
       " 'bustier': 845,\n",
       " 'busting': 846,\n",
       " 'bustline': 847,\n",
       " 'busts': 848,\n",
       " 'busty': 849,\n",
       " 'busy': 850,\n",
       " 'butt': 851,\n",
       " 'butter': 852,\n",
       " 'butterflies': 853,\n",
       " 'butterfly': 854,\n",
       " 'buttery': 855,\n",
       " 'button': 856,\n",
       " 'button-down': 857,\n",
       " 'button-front': 858,\n",
       " 'button-up': 859,\n",
       " 'buttondown': 860,\n",
       " 'buttoned': 861,\n",
       " 'buttoned-up': 862,\n",
       " 'buttonhole': 863,\n",
       " 'buttonholes': 864,\n",
       " 'buttoning': 865,\n",
       " 'buttons': 866,\n",
       " 'butts': 867,\n",
       " 'buy': 868,\n",
       " 'buyer': 869,\n",
       " 'buyers': 870,\n",
       " 'buyi': 871,\n",
       " 'buying': 872,\n",
       " 'byron': 873,\n",
       " 'c-cup': 874,\n",
       " 'c-d': 875,\n",
       " 'ca': 876,\n",
       " 'cable': 877,\n",
       " 'cacti': 878,\n",
       " 'cafe': 879,\n",
       " 'caftan': 880,\n",
       " 'cage': 881,\n",
       " 'cake': 882,\n",
       " 'cal': 883,\n",
       " 'calf': 884,\n",
       " 'cali': 885,\n",
       " 'california': 886,\n",
       " 'call': 887,\n",
       " 'called': 888,\n",
       " 'calling': 889,\n",
       " 'calls': 890,\n",
       " 'calves': 891,\n",
       " 'cam': 892,\n",
       " 'camel': 893,\n",
       " 'camera': 894,\n",
       " 'cami': 895,\n",
       " \"cami's\": 896,\n",
       " 'camis': 897,\n",
       " 'camisol': 898,\n",
       " 'camisole': 899,\n",
       " 'camisoles': 900,\n",
       " 'camo': 901,\n",
       " 'camouflage': 902,\n",
       " 'camouflages': 903,\n",
       " 'camouflaging': 904,\n",
       " 'camp': 905,\n",
       " 'campus': 906,\n",
       " 'cancel': 907,\n",
       " 'cancelled': 908,\n",
       " 'cancer': 909,\n",
       " 'canvas': 910,\n",
       " 'canvas-y': 911,\n",
       " 'cap': 912,\n",
       " 'cape': 913,\n",
       " 'capes': 914,\n",
       " 'capped': 915,\n",
       " 'capri': 916,\n",
       " 'capris': 917,\n",
       " 'capsule': 918,\n",
       " 'capture': 919,\n",
       " 'car': 920,\n",
       " 'caramel': 921,\n",
       " 'carbon': 922,\n",
       " 'card': 923,\n",
       " 'cardi': 924,\n",
       " 'cardigan': 925,\n",
       " 'cardigans': 926,\n",
       " 'cardio': 927,\n",
       " 'cardis': 928,\n",
       " 'care': 929,\n",
       " 'carefree': 930,\n",
       " 'careful': 931,\n",
       " 'carefully': 932,\n",
       " 'cares': 933,\n",
       " 'cargo': 934,\n",
       " 'cargos': 935,\n",
       " 'caribbean': 936,\n",
       " 'carissima': 937,\n",
       " 'carpet': 938,\n",
       " 'carried': 939,\n",
       " 'carries': 940,\n",
       " 'carry': 941,\n",
       " 'carrying': 942,\n",
       " 'cart': 943,\n",
       " 'cartonnier': 944,\n",
       " 'cartoon': 945,\n",
       " 'cartoonish': 946,\n",
       " 'cas': 947,\n",
       " 'cascade': 948,\n",
       " 'cascades': 949,\n",
       " 'case': 950,\n",
       " 'cases': 951,\n",
       " 'cash': 952,\n",
       " 'cashier': 953,\n",
       " 'cashmere': 954,\n",
       " 'casing': 955,\n",
       " 'casu': 956,\n",
       " 'casual': 957,\n",
       " 'casually': 958,\n",
       " 'cat': 959,\n",
       " 'catalog': 960,\n",
       " 'catalogue': 961,\n",
       " 'catch': 962,\n",
       " 'catcher': 963,\n",
       " 'catches': 964,\n",
       " 'catching': 965,\n",
       " 'catchy': 966,\n",
       " 'categorized': 967,\n",
       " 'category': 968,\n",
       " 'cats': 969,\n",
       " 'caught': 970,\n",
       " 'causal': 971,\n",
       " 'caused': 972,\n",
       " 'causing': 973,\n",
       " 'caution': 974,\n",
       " 'cautious': 975,\n",
       " 'cave': 976,\n",
       " 'caveat': 977,\n",
       " 'caved': 978,\n",
       " 'cc': 979,\n",
       " 'ceases': 980,\n",
       " 'cedar': 981,\n",
       " 'cehst': 982,\n",
       " 'celadon': 983,\n",
       " 'celebrate': 984,\n",
       " 'celebration': 985,\n",
       " 'cell': 986,\n",
       " 'cellophane': 987,\n",
       " 'cellulite': 988,\n",
       " 'center': 989,\n",
       " 'centered': 990,\n",
       " 'cents': 991,\n",
       " 'ceremony': 992,\n",
       " 'ch': 993,\n",
       " 'chain': 994,\n",
       " 'chair': 995,\n",
       " 'challenge': 996,\n",
       " 'challenged': 997,\n",
       " 'challenges': 998,\n",
       " 'challenging': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] w08_act2_embedding_classification.ipynb - 4. Generating TF-IDF weighted document vectors - cell 2\n",
    "def doc_wordweights(fName_tVectors, vocab):\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}  # Reversing the vocab dictionary to map indices to words for easy lookup\n",
    "    tfidf_weights = [] # initialising empty list\n",
    "\n",
    "    with open(fName_tVectors) as tVecf: # opening the tvector of file of reviews\n",
    "        tVectors = tVecf.read().splitlines()  # reading the file and splitting on each line\n",
    "\n",
    "    for tv in tVectors: # looping through each vector\n",
    "        tv = tv.strip()\n",
    "        weights = tv.split(' ')  # splitting on 'word_index:weight' entries\n",
    "\n",
    "        # filtering out any empty or improperly formatted entries\n",
    "        weights = [w.split(':') for w in weights if ':' in w and len(w.split(':')) == 2]\n",
    "\n",
    "        # constructing the word-weight dictionary \n",
    "        wordweight_dict = {inv_vocab[int(w[0])]: float(w[1]) for w in weights}\n",
    "        tfidf_weights.append(wordweight_dict)\n",
    "\n",
    "    return tfidf_weights # returning dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] w08_act2_embedding_classification.ipynb - 4. Generating TF-IDF weighted document vectors - cell 2\n",
    "# reading TF-IDF weights from the file\n",
    "fName_tVectors = 'reviews_tVector.txt'\n",
    "tfidf_weights = doc_wordweights(fName_tVectors, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zipper': 0.13796655643162234,\n",
       " 'zip': 0.15790936302533073,\n",
       " 'work': 0.08942754431903749,\n",
       " 'wanted': 0.10688230992054923,\n",
       " 'usual': 0.1108979967093157,\n",
       " 'tight': 0.09944270371682062,\n",
       " 'sewn': 0.15731076193390778,\n",
       " 'reordered': 0.20116611478729465,\n",
       " 'petite': 0.17993461419688872,\n",
       " 'outrageously': 0.2567966207221875,\n",
       " 'nicely': 0.11400095142158763,\n",
       " 'net': 0.4867876227942154,\n",
       " 'medium': 0.09045659947118836,\n",
       " 'major': 0.19468911285051188,\n",
       " 'layers': 0.16401760637664864,\n",
       " 'layer': 0.2738967869996706,\n",
       " 'initially': 0.17097793161240013,\n",
       " 'imo': 0.19309847172209627,\n",
       " 'hopes': 0.16844705121993458,\n",
       " 'high': 0.10509713403953778,\n",
       " 'half': 0.29992142257858656,\n",
       " 'found': 0.11016820762903567,\n",
       " 'flaw': 0.1869368578273477,\n",
       " 'fact': 0.14687037381367216,\n",
       " 'directly': 0.22042366265962396,\n",
       " 'design': 0.0981905331970358,\n",
       " 'cheap': 0.13834681733551207,\n",
       " 'bottom': 0.10490905048781156}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] w08_act2_embedding_classification.ipynb - 4. Generating TF-IDF weighted document vectors - cell 5\n",
    "def weighted_docvecs(embeddings, tfidf, docs):\n",
    "    vecs = np.zeros((len(docs), embeddings.vector_size)) # initialising empty matrix\n",
    "    for i, doc in enumerate(docs): # looping through each document and its index\n",
    "        valid_keys = [term for term in doc if term in embeddings.key_to_index] # filtering words that exist in embeddings vocabulary\n",
    "        tf_weights = [float(tfidf[i].get(term, 0.)) for term in valid_keys] # retrieving weights for for valid keys\n",
    "        \n",
    "        assert len(valid_keys) == len(tf_weights) # ensuring number of keys match the number of weights\n",
    "        \n",
    "        # calculating the weighted vectors\n",
    "        weighted = [embeddings[term] * w for term, w in zip(valid_keys, tf_weights)]\n",
    "        docvec = np.vstack(weighted) if weighted else np.zeros((1, embeddings.vector_size))  # handling empty cases\n",
    "        docvec = np.sum(docvec, axis=0) # summing to get final document vector\n",
    "        vecs[i, :] = docvec # storing the vector\n",
    "    return vecs # returning the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] w08_act2_embedding_classification.ipynb - 4. Generating TF-IDF weighted document vectors - cell 6\n",
    "weighted_preTW2v_dvs = weighted_docvecs(preTW2v_wv, tfidf_weights, review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81326997, -1.20487916,  1.61317408, -0.87714046,  1.54976892,\n",
       "        1.12291992, -0.74394214, -2.06082153,  0.89540523,  0.68368208,\n",
       "        0.71094972, -0.34214959, -0.86025536,  0.71007252,  0.35005403,\n",
       "        0.09028979,  0.06456479, -0.92537606, -0.66902792, -2.38735271,\n",
       "       -0.40709573, -2.00046849,  1.04969573, -0.57348299, -0.77753061,\n",
       "       -3.94294429, -1.26127577,  3.12087083,  2.28794479, -1.63063359,\n",
       "       13.12898445,  0.54430461,  2.23539066,  0.80154663, -0.30648628,\n",
       "        0.96228892, -0.87200916,  2.75626683,  1.20811796, -2.01931071,\n",
       "        0.88989699,  0.17840432,  1.45466089,  0.51977223, -2.35954976,\n",
       "       -0.30457258,  1.50499237,  0.35037398,  0.98145258, -0.72741616])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_preTW2v_dvs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Clothing Review Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building classification models based on different document feature represetations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] w08_act2_embedding_classification.ipynb - 4. Generating TF-IDF weighted document vectors - cell 8\n",
    "seed = 0 # setting a seed to make sure the output is reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Count Feature:  0.8749614554424915\n",
      "Accuracy of Unweighted Pretrained Word2Vec:  0.8259327782917052\n",
      "Accuracy of Weighted Pretrained Word2Vec:  0.8267036694418748\n"
     ]
    }
   ],
   "source": [
    "# [2] w08_act2_embedding_classification.ipynb - 4. Generating TF-IDF weighted document vectors - cell 8\n",
    "models = [count_features,unweighted_vec_rep,weighted_preTW2v_dvs]\n",
    "model_names = ['Count Feature','Unweighted Pretrained Word2Vec', 'Weighted Pretrained Word2Vec']\n",
    "for i in range(0,len(models)): #loop through each model\n",
    "    dv = models[i]\n",
    "    \n",
    "    # creating training and test split\n",
    "    X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(dv, clothes_review_data['Recommended IND'], list(range(0,len(clothes_review_data))),test_size=0.33, random_state=seed)\n",
    "\n",
    "    model = LogisticRegression(max_iter = 2000,random_state=seed) # initialising the model\n",
    "    model.fit(X_train, y_train) # training the model\n",
    "    \n",
    "    print(f\"Accuracy of {model_names[i]}: \", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "19647    1\n",
       "19648    1\n",
       "19649    0\n",
       "19650    1\n",
       "19651    1\n",
       "Name: Recommended IND, Length: 19652, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5] Act 4_Document Classification.ipynb\n",
    "labels = clothes_review_data['Recommended IND']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=0, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 36\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits= num_folds, random_state=seed, shuffle = True) # initialising a 5 fold validation\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 37\n",
    "def evaluate(X_train,X_test,y_train, y_test,seed):\n",
    "    model = LogisticRegression(max_iter = 500, random_state=seed) # initialising the model\n",
    "    model.fit(X_train, y_train) # training the model\n",
    "    return model.score(X_test, y_test) # returning the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 38\n",
    "num_models = 3\n",
    "cv_df = pd.DataFrame(columns = ['Count','Unweighted','Weighted'],index=range(num_folds)) # creating a dataframe to store the accuracy scores in all the folds\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(list(range(0,len(labels)))): # looping through each split\n",
    "    # extracting labels based on current fold\n",
    "    y_train = [labels[i] for i in train_index]\n",
    "    y_test = [labels[i] for i in test_index]\n",
    "\n",
    "    # Count Features Model Evaluation\n",
    "    X_train_count, X_test_count = count_features[train_index], count_features[test_index]\n",
    "    cv_df.loc[fold,'Count'] = evaluate(count_features[train_index],count_features[test_index],y_train,y_test,seed)\n",
    "\n",
    "    # Unweighted Vector Model Evaluation\n",
    "    X_train_unweighted, X_test_unweighted = unweighted_vec_rep[train_index], unweighted_vec_rep[test_index]\n",
    "    cv_df.loc[fold,'Unweighted'] = evaluate(unweighted_vec_rep[train_index],unweighted_vec_rep[test_index],y_train,y_test,seed)\n",
    "\n",
    "    # Weighted Vector Model Evaluation\n",
    "    X_train_weighted, X_test_weighted = weighted_preTW2v_dvs[train_index], weighted_preTW2v_dvs[test_index]\n",
    "    cv_df.loc[fold,'Weighted'] = evaluate(weighted_preTW2v_dvs[train_index],weighted_preTW2v_dvs[test_index],y_train,y_test,seed)\n",
    "    \n",
    "    fold +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Unweighted</th>\n",
       "      <th>Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.873823</td>\n",
       "      <td>0.823709</td>\n",
       "      <td>0.823963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.876113</td>\n",
       "      <td>0.828797</td>\n",
       "      <td>0.830069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.875573</td>\n",
       "      <td>0.825191</td>\n",
       "      <td>0.822646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.864122</td>\n",
       "      <td>0.816031</td>\n",
       "      <td>0.819593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.880153</td>\n",
       "      <td>0.83257</td>\n",
       "      <td>0.829771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Count Unweighted  Weighted\n",
       "0  0.873823   0.823709  0.823963\n",
       "1  0.876113   0.828797  0.830069\n",
       "2  0.875573   0.825191  0.822646\n",
       "3  0.864122   0.816031  0.819593\n",
       "4  0.880153    0.83257  0.829771"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 39\n",
    "cv_df # checking the accuracy score of each fold for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the column 'Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  Some major design flaws\n",
       "1                                         My favorite buy!\n",
       "2                                         Flattering shirt\n",
       "3                                  Not for the very petite\n",
       "4                                     Cagrcoal shimmer fun\n",
       "                               ...                        \n",
       "19647                       Great dress for many occasions\n",
       "19648                           Wish it was made of cotton\n",
       "19649                                Cute, but see through\n",
       "19650    Very cute dress, perfect for summer parties an...\n",
       "19651                      Please make more like this one!\n",
       "Name: Title, Length: 19652, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6]\n",
    "# extracting the 'Title' column for pre-processing\n",
    "title = clothes_review_data['Title']\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "# function for case normalisation and tokensisation\n",
    "def tokenize_reviews(review_text):\n",
    "    nl_review = review_text.lower()  # converting reviews to lowercase\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\" # regex pattern to tokenise the reviews\n",
    "    tokenizer = RegexpTokenizer(pattern) # tokeniser to split the reviews based on the regex pattern\n",
    "    tokenized_review = tokenizer.tokenize(nl_review) # passing normalised reviews to get list of tokens\n",
    "    return tokenized_review # returning cleaned list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             [some, major, design, flaws]\n",
       "1                                      [my, favorite, buy]\n",
       "2                                      [flattering, shirt]\n",
       "3                            [not, for, the, very, petite]\n",
       "4                                 [cagrcoal, shimmer, fun]\n",
       "                               ...                        \n",
       "19647                 [great, dress, for, many, occasions]\n",
       "19648                    [wish, it, was, made, of, cotton]\n",
       "19649                            [cute, but, see, through]\n",
       "19650    [very, cute, dress, perfect, for, summer, part...\n",
       "19651                [please, make, more, like, this, one]\n",
       "Name: Title, Length: 19652, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6]\n",
    "tokenized_title = title.apply(tokenize_reviews) # applying the function to get cleaned list of tokens\n",
    "tokenized_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "#loading the stopwords file\n",
    "stopwords_file = \"stopwords_en.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "# reading the file \n",
    "with open(stopwords_file, 'r') as file: # opens file in read mode\n",
    "    stopwords = file.read().splitlines() # splits into line such that there is one stopwword per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6]\n",
    "# getting unique stopwords\n",
    "stopwords = set(stopwords)\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "# function for removal of words\n",
    "def remove_words(tokens, stopwords):\n",
    "    tokens = [token for token in tokens if len(token) >= 2] # removing words with the length less than 2\n",
    "    tokens = [token for token in tokens if token not in stopwords] # removing stopwords \n",
    "    return tokens # returning cleaned list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [major, design, flaws]\n",
       "1                                [favorite, buy]\n",
       "2                            [flattering, shirt]\n",
       "3                                       [petite]\n",
       "4                       [cagrcoal, shimmer, fun]\n",
       "                          ...                   \n",
       "19647                  [great, dress, occasions]\n",
       "19648                             [made, cotton]\n",
       "19649                                     [cute]\n",
       "19650    [cute, dress, perfect, summer, parties]\n",
       "19651                                     [make]\n",
       "Name: Title, Length: 19652, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6]\n",
    "# applying the function on tokenized reviews to remove the words on said condition\n",
    "cleaned_title = tokenized_title.apply(lambda tokens: remove_words(tokens, stopwords))\n",
    "cleaned_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 11\n",
    "words = list(chain.from_iterable(cleaned_title)) # we put all the tokens in the corpus in a single list\n",
    "vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'love': 1864, 'great': 1790, 'dress': 1648, 'cute': 1553, 'beautiful': 1405, 'top': 1172, 'perfect': 815, 'pretty': 672, 'fit': 611, 'nice': 526, ...})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6]\n",
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 12\n",
    "term_fd = FreqDist(words) # compute term frequency for each unique word/type\n",
    "term_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [major, design, flaws]\n",
       "1                       [favorite, buy]\n",
       "2                   [flattering, shirt]\n",
       "3                              [petite]\n",
       "4                        [shimmer, fun]\n",
       "                      ...              \n",
       "19647         [great, dress, occasions]\n",
       "19648                    [made, cotton]\n",
       "19649                            [cute]\n",
       "19650    [cute, dress, perfect, summer]\n",
       "19651                            [make]\n",
       "Name: Title, Length: 19652, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6]\n",
    "# removing the words that only appear once in the document collection\n",
    "term_freq = cleaned_title.apply(lambda tokens: [word for word in tokens if term_fd[word] > 1])\n",
    "term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'great': 1771, 'love': 1674, 'dress': 1640, 'cute': 1543, 'beautiful': 1398, 'top': 1172, 'perfect': 815, 'pretty': 669, 'fit': 611, 'nice': 525, ...})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6] \n",
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 15\n",
    "words_2 = list(chain.from_iterable([set(review) for review in term_freq]))\n",
    "doc_fd = FreqDist(words_2)  # compute document frequency for each unique word/type\n",
    "doc_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'love',\n",
       " 'dress',\n",
       " 'cute',\n",
       " 'beautiful',\n",
       " 'top',\n",
       " 'perfect',\n",
       " 'pretty',\n",
       " 'fit',\n",
       " 'nice',\n",
       " 'flattering',\n",
       " 'runs',\n",
       " 'comfortable',\n",
       " 'lovely',\n",
       " 'comfy',\n",
       " 'gorgeous',\n",
       " 'summer',\n",
       " 'soft',\n",
       " 'sweater',\n",
       " 'small']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6] \n",
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 15\n",
    "top20_freq_words =doc_fd.most_common(20)\n",
    "top20_freq_words = [word for word, freq in top20_freq_words]\n",
    "top20_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [major, design, flaws]\n",
       "1               [favorite, buy]\n",
       "2                       [shirt]\n",
       "3                      [petite]\n",
       "4                [shimmer, fun]\n",
       "                  ...          \n",
       "19647               [occasions]\n",
       "19648            [made, cotton]\n",
       "19649                        []\n",
       "19650                        []\n",
       "19651                    [make]\n",
       "Name: Title, Length: 19652, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6] \n",
    "# removing the top 20 most frequent words\n",
    "processed_title = term_freq.apply(lambda tokens: [word for word in tokens if word not in top20_freq_words])\n",
    "processed_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "# statistics on processed titles\n",
    "def stats_print(tokenized_reviews):\n",
    "    words = list(chain.from_iterable(tokenized_reviews)) # we put all the tokens in the corpus in a single list\n",
    "    vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of reviews:\", len(tokenized_reviews))\n",
    "    lens = [len(article) for article in tokenized_reviews]\n",
    "    print(\"Average document length:\", np.mean(lens))\n",
    "    print(\"Maximum document length:\", np.max(lens))\n",
    "    print(\"Minimum document length:\", np.min(lens))\n",
    "    print(\"Standard deviation of document length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1608\n",
      "Total number of tokens:  24611\n",
      "Lexical diversity:  0.06533663808865954\n",
      "Total number of reviews: 19652\n",
      "Average document length: 1.252340728679015\n",
      "Maximum document length: 6\n",
      "Minimum document length: 0\n",
      "Standard deviation of document length: 1.0078978352641619\n"
     ]
    }
   ],
   "source": [
    "# [6]\n",
    "stats_print(processed_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4718"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6]\n",
    "# noticed that minimum document length is 0, hence, computed the number of empty lists\n",
    "empty_lists = processed_title.apply(lambda x: len(x) == 0).sum()\n",
    "empty_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "# appending processed_title column to the original dataframe\n",
    "clothes_review_data['Processed Title'] = processed_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Processed Review Text</th>\n",
       "      <th>Processed Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[high, hopes, wanted, work, initially, petite,...</td>\n",
       "      <td>[major, design, flaws]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>[jumpsuit, fun, flirty, fabulous, time, compli...</td>\n",
       "      <td>[favorite, buy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>[shirt, due, adjustable, front, tie, length, l...</td>\n",
       "      <td>[shirt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[tracy, reese, dresses, petite, feet, tall, br...</td>\n",
       "      <td>[petite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[basket, hte, person, store, pick, teh, pale, ...</td>\n",
       "      <td>[shimmer, fun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[happy, snag, price, easy, slip, cut, combo]</td>\n",
       "      <td>[occasions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[reminds, maternity, clothes, stretchy, shiny,...</td>\n",
       "      <td>[made, cotton]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19649</th>\n",
       "      <td>1104</td>\n",
       "      <td>31</td>\n",
       "      <td>Cute, but see through</td>\n",
       "      <td>This fit well, but the top was very see throug...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[worked, glad, store, order, online]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19650</th>\n",
       "      <td>1084</td>\n",
       "      <td>28</td>\n",
       "      <td>Very cute dress, perfect for summer parties an...</td>\n",
       "      <td>I bought this dress for a wedding i have this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[wedding, summer, medium, waist, perfectly, lo...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19651</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[lovely, feminine, perfectly, easy, comfy, hig...</td>\n",
       "      <td>[make]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19652 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                                              Title  \\\n",
       "0             1077   60                            Some major design flaws   \n",
       "1             1049   50                                   My favorite buy!   \n",
       "2              847   47                                   Flattering shirt   \n",
       "3             1080   49                            Not for the very petite   \n",
       "4              858   39                               Cagrcoal shimmer fun   \n",
       "...            ...  ...                                                ...   \n",
       "19647         1104   34                     Great dress for many occasions   \n",
       "19648          862   48                         Wish it was made of cotton   \n",
       "19649         1104   31                              Cute, but see through   \n",
       "19650         1084   28  Very cute dress, perfect for summer parties an...   \n",
       "19651         1104   52                    Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "19647  I was very happy to snag this dress at such a ...       5   \n",
       "19648  It reminds me of maternity clothes. soft, stre...       3   \n",
       "19649  This fit well, but the top was very see throug...       3   \n",
       "19650  I bought this dress for a wedding i have this ...       3   \n",
       "19651  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "19647                1                        0  General Petite   \n",
       "19648                1                        0  General Petite   \n",
       "19649                0                        1  General Petite   \n",
       "19650                1                        2         General   \n",
       "19651                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name  \\\n",
       "0             Dresses    Dresses   \n",
       "1             Bottoms      Pants   \n",
       "2                Tops    Blouses   \n",
       "3             Dresses    Dresses   \n",
       "4                Tops      Knits   \n",
       "...               ...        ...   \n",
       "19647         Dresses    Dresses   \n",
       "19648            Tops      Knits   \n",
       "19649         Dresses    Dresses   \n",
       "19650         Dresses    Dresses   \n",
       "19651         Dresses    Dresses   \n",
       "\n",
       "                                   Processed Review Text  \\\n",
       "0      [high, hopes, wanted, work, initially, petite,...   \n",
       "1      [jumpsuit, fun, flirty, fabulous, time, compli...   \n",
       "2      [shirt, due, adjustable, front, tie, length, l...   \n",
       "3      [tracy, reese, dresses, petite, feet, tall, br...   \n",
       "4      [basket, hte, person, store, pick, teh, pale, ...   \n",
       "...                                                  ...   \n",
       "19647       [happy, snag, price, easy, slip, cut, combo]   \n",
       "19648  [reminds, maternity, clothes, stretchy, shiny,...   \n",
       "19649               [worked, glad, store, order, online]   \n",
       "19650  [wedding, summer, medium, waist, perfectly, lo...   \n",
       "19651  [lovely, feminine, perfectly, easy, comfy, hig...   \n",
       "\n",
       "              Processed Title  \n",
       "0      [major, design, flaws]  \n",
       "1             [favorite, buy]  \n",
       "2                     [shirt]  \n",
       "3                    [petite]  \n",
       "4              [shimmer, fun]  \n",
       "...                       ...  \n",
       "19647             [occasions]  \n",
       "19648          [made, cotton]  \n",
       "19649                      []  \n",
       "19650                      []  \n",
       "19651                  [make]  \n",
       "\n",
       "[19652 rows x 12 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clothes_review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6] \n",
    "# dropping the rows with empty lists\n",
    "df_cleaned = clothes_review_data[clothes_review_data['Processed Title'].apply(lambda x: len(x) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6] \n",
    "# cross verifying if there exists any empty titles in the cleaned dataframe\n",
    "empty_lists = df_cleaned['Processed Title'].apply(lambda x: len(x) == 0).sum()\n",
    "empty_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1608\n",
      "Total number of tokens:  24611\n",
      "Lexical diversity:  0.06533663808865954\n",
      "Total number of reviews: 14934\n",
      "Average document length: 1.647984464979242\n",
      "Maximum document length: 6\n",
      "Minimum document length: 1\n",
      "Standard deviation of document length: 0.8275123197711113\n"
     ]
    }
   ],
   "source": [
    "# [6] \n",
    "# statistics on processed titles\n",
    "stats_print(df_cleaned['Processed Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reset the index of the DataFrame because, when I drop rows with empty reviews, their corresponding indices are also removed. I encountered an error while splitting the data into train and test becuause the the train_test_split splits on index and there were missing indices. It's good practice to reset the index before exporting the data to maintain readability and avoid potential issues in future tasks. A sequential index ensures clarity and consistency in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6] \n",
    "# resetting the index of the dataframe\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Processed Review Text</th>\n",
       "      <th>Processed Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[high, hopes, wanted, work, initially, petite,...</td>\n",
       "      <td>[major, design, flaws]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>[jumpsuit, fun, flirty, fabulous, time, compli...</td>\n",
       "      <td>[favorite, buy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>[shirt, due, adjustable, front, tie, length, l...</td>\n",
       "      <td>[shirt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[tracy, reese, dresses, petite, feet, tall, br...</td>\n",
       "      <td>[petite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[basket, hte, person, store, pick, teh, pale, ...</td>\n",
       "      <td>[shimmer, fun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14929</th>\n",
       "      <td>1104</td>\n",
       "      <td>32</td>\n",
       "      <td>Unflattering</td>\n",
       "      <td>I was surprised at the positive reviews for th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[surprised, positive, reviews, product, terrib...</td>\n",
       "      <td>[unflattering]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14930</th>\n",
       "      <td>1005</td>\n",
       "      <td>42</td>\n",
       "      <td>What a fun piece!</td>\n",
       "      <td>So i wasn't sure about ordering this skirt bec...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>[ordering, skirt, person, glad, skirt, design,...</td>\n",
       "      <td>[fun, piece]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14931</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[happy, snag, price, easy, slip, cut, combo]</td>\n",
       "      <td>[occasions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14932</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[reminds, maternity, clothes, stretchy, shiny,...</td>\n",
       "      <td>[made, cotton]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14933</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[lovely, feminine, perfectly, easy, comfy, hig...</td>\n",
       "      <td>[make]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14934 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                            Title  \\\n",
       "0             1077   60          Some major design flaws   \n",
       "1             1049   50                 My favorite buy!   \n",
       "2              847   47                 Flattering shirt   \n",
       "3             1080   49          Not for the very petite   \n",
       "4              858   39             Cagrcoal shimmer fun   \n",
       "...            ...  ...                              ...   \n",
       "14929         1104   32                     Unflattering   \n",
       "14930         1005   42                What a fun piece!   \n",
       "14931         1104   34   Great dress for many occasions   \n",
       "14932          862   48       Wish it was made of cotton   \n",
       "14933         1104   52  Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "14929  I was surprised at the positive reviews for th...       1   \n",
       "14930  So i wasn't sure about ordering this skirt bec...       5   \n",
       "14931  I was very happy to snag this dress at such a ...       5   \n",
       "14932  It reminds me of maternity clothes. soft, stre...       3   \n",
       "14933  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "14929                0                        0  General Petite   \n",
       "14930                1                        0  General Petite   \n",
       "14931                1                        0  General Petite   \n",
       "14932                1                        0  General Petite   \n",
       "14933                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name  \\\n",
       "0             Dresses    Dresses   \n",
       "1             Bottoms      Pants   \n",
       "2                Tops    Blouses   \n",
       "3             Dresses    Dresses   \n",
       "4                Tops      Knits   \n",
       "...               ...        ...   \n",
       "14929         Dresses    Dresses   \n",
       "14930         Bottoms     Skirts   \n",
       "14931         Dresses    Dresses   \n",
       "14932            Tops      Knits   \n",
       "14933         Dresses    Dresses   \n",
       "\n",
       "                                   Processed Review Text  \\\n",
       "0      [high, hopes, wanted, work, initially, petite,...   \n",
       "1      [jumpsuit, fun, flirty, fabulous, time, compli...   \n",
       "2      [shirt, due, adjustable, front, tie, length, l...   \n",
       "3      [tracy, reese, dresses, petite, feet, tall, br...   \n",
       "4      [basket, hte, person, store, pick, teh, pale, ...   \n",
       "...                                                  ...   \n",
       "14929  [surprised, positive, reviews, product, terrib...   \n",
       "14930  [ordering, skirt, person, glad, skirt, design,...   \n",
       "14931       [happy, snag, price, easy, slip, cut, combo]   \n",
       "14932  [reminds, maternity, clothes, stretchy, shiny,...   \n",
       "14933  [lovely, feminine, perfectly, easy, comfy, hig...   \n",
       "\n",
       "              Processed Title  \n",
       "0      [major, design, flaws]  \n",
       "1             [favorite, buy]  \n",
       "2                     [shirt]  \n",
       "3                    [petite]  \n",
       "4              [shimmer, fun]  \n",
       "...                       ...  \n",
       "14929          [unflattering]  \n",
       "14930            [fun, piece]  \n",
       "14931             [occasions]  \n",
       "14932          [made, cotton]  \n",
       "14933                  [make]  \n",
       "\n",
       "[14934 rows x 12 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 47\n",
    "count_features_title = cVectorizer.fit_transform([' '.join(title) for title in df_cleaned['Processed Title']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14934, 7529)\n"
     ]
    }
   ],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 48\n",
    "print(count_features_title.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1716)\t1\n",
      "  (0, 2485)\t1\n",
      "  (0, 3832)\t1\n",
      "  (1, 868)\t1\n",
      "  (1, 2347)\t1\n",
      "  (2, 5736)\t1\n",
      "  (3, 4639)\t1\n",
      "  (4, 2667)\t1\n",
      "  (4, 5720)\t1\n",
      "  (5, 3750)\t1\n",
      "  (5, 5720)\t1\n",
      "  (5, 6458)\t1\n",
      "  (6, 2667)\t1\n",
      "  (7, 1028)\t1\n",
      "  (7, 3808)\t1\n",
      "  (7, 3893)\t1\n",
      "  (8, 537)\t1\n",
      "  (9, 3343)\t1\n",
      "  (9, 4550)\t1\n",
      "  (10, 635)\t1\n",
      "  (11, 362)\t1\n",
      "  (11, 2976)\t1\n",
      "  (11, 6549)\t1\n",
      "  (12, 4528)\t1\n",
      "  (12, 7325)\t1\n",
      "  :\t:\n",
      "  (14915, 5900)\t1\n",
      "  (14916, 2232)\t1\n",
      "  (14917, 98)\t1\n",
      "  (14918, 5945)\t1\n",
      "  (14919, 3010)\t1\n",
      "  (14919, 3087)\t1\n",
      "  (14920, 4528)\t1\n",
      "  (14921, 1287)\t1\n",
      "  (14922, 2313)\t1\n",
      "  (14922, 4234)\t1\n",
      "  (14923, 6430)\t1\n",
      "  (14924, 6327)\t1\n",
      "  (14924, 6492)\t1\n",
      "  (14925, 375)\t1\n",
      "  (14926, 1924)\t1\n",
      "  (14926, 4621)\t1\n",
      "  (14927, 4628)\t1\n",
      "  (14928, 1041)\t1\n",
      "  (14929, 7016)\t1\n",
      "  (14930, 2667)\t1\n",
      "  (14930, 4675)\t1\n",
      "  (14931, 4318)\t1\n",
      "  (14932, 1402)\t1\n",
      "  (14932, 3808)\t1\n",
      "  (14933, 3835)\t1\n"
     ]
    }
   ],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 48\n",
    "print(count_features_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8488537228646784"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 34\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(count_features_title, df_cleaned['Recommended IND'], list(range(0,len(df_cleaned))),test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(max_iter = 500,random_state=seed) # initialising the model\n",
    "model.fit(X_train, y_train) # training the model\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14934"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_title = df_cleaned['Recommended IND']\n",
    "len(labels_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 38\n",
    "cv_df = pd.DataFrame(columns = ['count'],index=range(num_folds)) # creates a dataframe to store the accuracy scores in all the folds\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(list(range(0,len(labels_title)))): # looping through each split\n",
    "\n",
    "    # extracting labels based on current fold\n",
    "    y_train = [labels_title[i] for i in train_index]\n",
    "    y_test = [labels_title[i] for i in test_index]\n",
    "\n",
    "    # Count Features Model Evaluation\n",
    "    X_train_count, X_test_count = count_features_title[train_index], count_features_title[test_index]\n",
    "    cv_df.loc[fold,'count'] = evaluate(count_features_title[train_index],count_features_title[test_index],y_train,y_test,seed)\n",
    "    \n",
    "    fold +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.860395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.851356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.852025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.85303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.858339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count\n",
       "0  0.860395\n",
       "1  0.851356\n",
       "2  0.852025\n",
       "3   0.85303\n",
       "4  0.858339"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 39\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['Combined'] = df_cleaned['Processed Review Text'] + df_cleaned['Processed Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Processed Review Text</th>\n",
       "      <th>Processed Title</th>\n",
       "      <th>Combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[high, hopes, wanted, work, initially, petite,...</td>\n",
       "      <td>[major, design, flaws]</td>\n",
       "      <td>[high, hopes, wanted, work, initially, petite,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>[jumpsuit, fun, flirty, fabulous, time, compli...</td>\n",
       "      <td>[favorite, buy]</td>\n",
       "      <td>[jumpsuit, fun, flirty, fabulous, time, compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>[shirt, due, adjustable, front, tie, length, l...</td>\n",
       "      <td>[shirt]</td>\n",
       "      <td>[shirt, due, adjustable, front, tie, length, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[tracy, reese, dresses, petite, feet, tall, br...</td>\n",
       "      <td>[petite]</td>\n",
       "      <td>[tracy, reese, dresses, petite, feet, tall, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[basket, hte, person, store, pick, teh, pale, ...</td>\n",
       "      <td>[shimmer, fun]</td>\n",
       "      <td>[basket, hte, person, store, pick, teh, pale, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14929</th>\n",
       "      <td>1104</td>\n",
       "      <td>32</td>\n",
       "      <td>Unflattering</td>\n",
       "      <td>I was surprised at the positive reviews for th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[surprised, positive, reviews, product, terrib...</td>\n",
       "      <td>[unflattering]</td>\n",
       "      <td>[surprised, positive, reviews, product, terrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14930</th>\n",
       "      <td>1005</td>\n",
       "      <td>42</td>\n",
       "      <td>What a fun piece!</td>\n",
       "      <td>So i wasn't sure about ordering this skirt bec...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>[ordering, skirt, person, glad, skirt, design,...</td>\n",
       "      <td>[fun, piece]</td>\n",
       "      <td>[ordering, skirt, person, glad, skirt, design,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14931</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[happy, snag, price, easy, slip, cut, combo]</td>\n",
       "      <td>[occasions]</td>\n",
       "      <td>[happy, snag, price, easy, slip, cut, combo, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14932</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[reminds, maternity, clothes, stretchy, shiny,...</td>\n",
       "      <td>[made, cotton]</td>\n",
       "      <td>[reminds, maternity, clothes, stretchy, shiny,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14933</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[lovely, feminine, perfectly, easy, comfy, hig...</td>\n",
       "      <td>[make]</td>\n",
       "      <td>[lovely, feminine, perfectly, easy, comfy, hig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14934 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                            Title  \\\n",
       "0             1077   60          Some major design flaws   \n",
       "1             1049   50                 My favorite buy!   \n",
       "2              847   47                 Flattering shirt   \n",
       "3             1080   49          Not for the very petite   \n",
       "4              858   39             Cagrcoal shimmer fun   \n",
       "...            ...  ...                              ...   \n",
       "14929         1104   32                     Unflattering   \n",
       "14930         1005   42                What a fun piece!   \n",
       "14931         1104   34   Great dress for many occasions   \n",
       "14932          862   48       Wish it was made of cotton   \n",
       "14933         1104   52  Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "14929  I was surprised at the positive reviews for th...       1   \n",
       "14930  So i wasn't sure about ordering this skirt bec...       5   \n",
       "14931  I was very happy to snag this dress at such a ...       5   \n",
       "14932  It reminds me of maternity clothes. soft, stre...       3   \n",
       "14933  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "14929                0                        0  General Petite   \n",
       "14930                1                        0  General Petite   \n",
       "14931                1                        0  General Petite   \n",
       "14932                1                        0  General Petite   \n",
       "14933                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name  \\\n",
       "0             Dresses    Dresses   \n",
       "1             Bottoms      Pants   \n",
       "2                Tops    Blouses   \n",
       "3             Dresses    Dresses   \n",
       "4                Tops      Knits   \n",
       "...               ...        ...   \n",
       "14929         Dresses    Dresses   \n",
       "14930         Bottoms     Skirts   \n",
       "14931         Dresses    Dresses   \n",
       "14932            Tops      Knits   \n",
       "14933         Dresses    Dresses   \n",
       "\n",
       "                                   Processed Review Text  \\\n",
       "0      [high, hopes, wanted, work, initially, petite,...   \n",
       "1      [jumpsuit, fun, flirty, fabulous, time, compli...   \n",
       "2      [shirt, due, adjustable, front, tie, length, l...   \n",
       "3      [tracy, reese, dresses, petite, feet, tall, br...   \n",
       "4      [basket, hte, person, store, pick, teh, pale, ...   \n",
       "...                                                  ...   \n",
       "14929  [surprised, positive, reviews, product, terrib...   \n",
       "14930  [ordering, skirt, person, glad, skirt, design,...   \n",
       "14931       [happy, snag, price, easy, slip, cut, combo]   \n",
       "14932  [reminds, maternity, clothes, stretchy, shiny,...   \n",
       "14933  [lovely, feminine, perfectly, easy, comfy, hig...   \n",
       "\n",
       "              Processed Title  \\\n",
       "0      [major, design, flaws]   \n",
       "1             [favorite, buy]   \n",
       "2                     [shirt]   \n",
       "3                    [petite]   \n",
       "4              [shimmer, fun]   \n",
       "...                       ...   \n",
       "14929          [unflattering]   \n",
       "14930            [fun, piece]   \n",
       "14931             [occasions]   \n",
       "14932          [made, cotton]   \n",
       "14933                  [make]   \n",
       "\n",
       "                                                Combined  \n",
       "0      [high, hopes, wanted, work, initially, petite,...  \n",
       "1      [jumpsuit, fun, flirty, fabulous, time, compli...  \n",
       "2      [shirt, due, adjustable, front, tie, length, l...  \n",
       "3      [tracy, reese, dresses, petite, feet, tall, br...  \n",
       "4      [basket, hte, person, store, pick, teh, pale, ...  \n",
       "...                                                  ...  \n",
       "14929  [surprised, positive, reviews, product, terrib...  \n",
       "14930  [ordering, skirt, person, glad, skirt, design,...  \n",
       "14931  [happy, snag, price, easy, slip, cut, combo, o...  \n",
       "14932  [reminds, maternity, clothes, stretchy, shiny,...  \n",
       "14933  [lovely, feminine, perfectly, easy, comfy, hig...  \n",
       "\n",
       "[14934 rows x 13 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 47\n",
    "count_features_combined = cVectorizer.fit_transform([' '.join(title) for title in df_cleaned['Combined']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14934, 7529)\n"
     ]
    }
   ],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 48\n",
    "print(count_features_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 687)\t1\n",
      "  (0, 1028)\t1\n",
      "  (0, 1716)\t2\n",
      "  (0, 1792)\t1\n",
      "  (0, 2289)\t1\n",
      "  (0, 2481)\t1\n",
      "  (0, 2485)\t1\n",
      "  (0, 2602)\t1\n",
      "  (0, 2892)\t2\n",
      "  (0, 3010)\t1\n",
      "  (0, 3087)\t1\n",
      "  (0, 3193)\t1\n",
      "  (0, 3258)\t1\n",
      "  (0, 3549)\t2\n",
      "  (0, 3552)\t1\n",
      "  (0, 3832)\t2\n",
      "  (0, 3934)\t1\n",
      "  (0, 4224)\t2\n",
      "  (0, 4234)\t1\n",
      "  (0, 4427)\t1\n",
      "  (0, 4639)\t2\n",
      "  (0, 5260)\t1\n",
      "  (0, 5668)\t1\n",
      "  (0, 6726)\t1\n",
      "  (0, 7092)\t1\n",
      "  :\t:\n",
      "  (14932, 1155)\t1\n",
      "  (14932, 1168)\t1\n",
      "  (14932, 1402)\t1\n",
      "  (14932, 1543)\t1\n",
      "  (14932, 1923)\t1\n",
      "  (14932, 2602)\t1\n",
      "  (14932, 2646)\t1\n",
      "  (14932, 3707)\t1\n",
      "  (14932, 3721)\t1\n",
      "  (14932, 3808)\t1\n",
      "  (14932, 3893)\t1\n",
      "  (14932, 3897)\t1\n",
      "  (14932, 4234)\t1\n",
      "  (14932, 5245)\t1\n",
      "  (14932, 5729)\t1\n",
      "  (14932, 5925)\t1\n",
      "  (14932, 6315)\t1\n",
      "  (14933, 1246)\t1\n",
      "  (14933, 2020)\t1\n",
      "  (14933, 2382)\t1\n",
      "  (14933, 3026)\t1\n",
      "  (14933, 3761)\t1\n",
      "  (14933, 3835)\t1\n",
      "  (14933, 4621)\t1\n",
      "  (14933, 5153)\t1\n"
     ]
    }
   ],
   "source": [
    "# [3] w07_act1_gen_feat_vec.ipynb - cell 48\n",
    "print(count_features_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8738080746601745"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 34\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(count_features_combined, df_cleaned['Recommended IND'], list(range(0,len(df_cleaned))),test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(max_iter = 500,random_state=seed) # initialising the model\n",
    "model.fit(X_train, y_train) # training the model\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14934"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_title = df_cleaned['Recommended IND']\n",
    "len(labels_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 38\n",
    "cv_df = pd.DataFrame(columns = ['count'],index=range(num_folds)) # creating a dataframe to store the accuracy scores in all the folds\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(list(range(0,len(labels_title)))):  # looping through each split\n",
    "    # extracting labels based on current fold\n",
    "    y_train = [labels_title[i] for i in train_index]\n",
    "    y_test = [labels_title[i] for i in test_index]\n",
    "\n",
    "    # Count Features Model Evaluation\n",
    "    X_train_count, X_test_count = count_features_combined[train_index], count_features_combined[test_index]\n",
    "    cv_df.loc[fold,'count'] = evaluate(count_features_combined[train_index],count_features_combined[test_index],y_train,y_test,seed)\n",
    "\n",
    "    fold +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.879143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.873117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.873786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.881152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.879437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count\n",
       "0  0.879143\n",
       "1  0.873117\n",
       "2  0.873786\n",
       "3  0.881152\n",
       "4  0.879437"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5] Act 4_Document Classification.ipynb - cell 39\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Language model comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model comparison results indicate that the Count representation consistently delivers better performance than both the Unweighted and Weighted pre-trained glove representations in classifying clothing reviews. Over five evaluation folds, the Count model achieved the highest accuracy scores, underscoring its effectiveness in capturing relevant features from the review texts. In contrast, the Unweighted and Weighted representations demonstrated similar levels of performance, with the Unweighted model slightly outperforming the Weighted model in certain cases.This indicates that the Count-based model is the most effective for this dataset, with the Weighted and Unweighted representations trailing behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Does more information provide higher accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task 2, I generated various feature representations for clothing reviews, but I did not take into account other features like the review titles. To determine if adding more information could enhance model accuracy, I performed experiments comparing the performance of classification models based on different input types: the title of the review alone, the review text, and a combination of both. I chose Count Vector Model to make the comparisons.<br>\n",
    "\n",
    "The results indicate that using only the title resulted in moderate accuracy scores, with a maximum accuracy of 0.860395 across five folds. In contrast, the review text alone achieved higher accuracy, peaking at 0.880153. However, when both the title and detailed reviews were combined, the accuracy improved further, with the highest accuracy recorded at 0.881152. <br>\n",
    "\n",
    "These results indicate that incorporating the title along with the review text improves classification performance, emphasizing the significance of utilizing all available information to boost model accuracy. I still think that if we had more number of tokens in Title column, we could have seen a significant improvement in the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This part of the assignment primarily focuses on analyzing clothing reviews through feature representation and classification. The steps involved in developing the code helped in understanding the key concepts of word embeddings and vector representation. The activities and lecture slides are designed precisely to handle the required tasks efficiently. The experiments conducted to compare different language models and to explore the impact of incorporating additional information, such as review titles, on model accuracy using 5-fold cross-validation for robust evaluation are thought provoking. This assessment provides a practical opportunity to apply machine learning techniques to natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Usage of eval(): https://stackoverflow.com/questions/67323995/how-to-remove-quotes-from-a-list-of-dataframes-in-python <br>\n",
    "[2] Canvas/Modules/Week 8 - Activities/w08_activities/w08_act2_embedding_classification.ipynb https://rmit.instructure.com/courses/125024/pages/week-8-activities-2?module_item_id=6449435 <br>\n",
    "[3] Canvas/Modules/Week 7 - Activities/w07_activities/w07_act1_gen_feat_vec.ipynb https://rmit.instructure.com/courses/125024/pages/week-7-activities-2?module_item_id=6449422 <br>\n",
    "[4] Canvas/Modules/Week 8/Extra Material/Generate Feature Vectors/Act 3_ Generating Feature Vectors.ipynb https://rmit.instructure.com/courses/125024/pages/generate-feature-vectors?module_item_id=6449431 <br>\n",
    "[5] Canvas/Modules/Week 8/Extra Material/Activity 4: Document Classification/Act 4_Document Classification.ipynb https://rmit.instructure.com/courses/125024/pages/activity-4-document-classification?module_item_id=6449432 <br>\n",
    "[6] task1.ipynb <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
